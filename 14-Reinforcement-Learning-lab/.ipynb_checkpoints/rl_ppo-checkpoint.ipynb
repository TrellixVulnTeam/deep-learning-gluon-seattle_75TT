{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](support/agent_env.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals of Reinforcement Learning\n",
    "\n",
    "Extend learning to cover cases where:\n",
    "\n",
    "* IID assumption does not hold\n",
    "* Prediction changes real world events\n",
    "* Try out new things and learn from experience\n",
    "* Need to stitch together sequence of steps to achieve prespecified goal(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Supervised Learning vs Reinforcement Learning\n",
    "\n",
    "|Supervised Learning | Reinforcement Learning |\n",
    "|------------------- |------------------------|\n",
    "|Input data, predict labels | Input observations, predict actions|\n",
    "|Minimize loss | Maximize rewards|\n",
    "|Model is relationship between data and label| Model is relationship between (states, actions) to new (states, rewards)|\n",
    "|Learn good model | Learn good policy (model, value function are optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reinforcement Learning Components\n",
    "\n",
    "\n",
    "### Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Agent: Plan and act in an environment. \n",
    "* achive a desired goal or maximize reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Environment: Context where the agent lives. \n",
    "* Provide an agent with observations\n",
    "* accept an agent action\n",
    "* change based on the agent action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sets\n",
    "\n",
    "State: Snapshot that completely specifies the environment at a point in time. \n",
    "\n",
    "* The set of all possible space is the state space. Can be discrete or continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Observation: Agents perception of the environment\n",
    "\n",
    "* May be incomplete and/or noisy versions of the state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Action: How the agents interacts to change the environment. \n",
    "\n",
    "* Set of all possible actions is the action space. Can also be discrete or continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Functions\n",
    "\n",
    "Model: A function that specifies the environment dynamics\n",
    "* Given a current state and an action, returns new state\n",
    "* Sometimes returns other information and the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reward: A function that specifies what the return is on an agents action.\n",
    "* Given a current state and an action, returns the reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(4,)\n",
      "action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0191488  -0.0122565   0.01730884 -0.03437213]\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](support/cartpole1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://gym.openai.com/envs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RL Formalization: Markov Decision Process\n",
    "\n",
    "Markov decision process is a 4-tuple $(S,A,P_{a},r_{a})$, where\n",
    "* $S$ : a finite set of states,\n",
    "* $A$ : a finite set of actions\n",
    "* $P_{a}(s,s') = \\Pr(s_{t+1}=s'\\mid s_{t}=s,a_{t}=a)$: probability that action $a$ in state $s$ leads to state $s'$\n",
    "* $r_{a}(s,s')$: the immediate reward received after transitioning from state $s$ to state $s'$, due to action $a$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](support/mdp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](support/trajectories.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Defining the Objective\n",
    "\n",
    "Find policy $\\pi(s)$ that maximizes total returns for an episode\n",
    "\n",
    "Define Return **$R_t$** as the sum of discounted rewards\n",
    "\n",
    "$$ R_t = r_t + \\gamma r_{t+1} + \\gamma r_{t+2} + \\gamma r_{t+3} + ... = \\sum_{k=0}^\\infty \\gamma^k r_{t+k}$$\n",
    "\n",
    "Return at t = 0, equals total expected at initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solving the MDP\n",
    "\n",
    "If all of $(S,A,P_{a},r_{a})$ are known we can solve the MDP to compute the policy $\\pi(s)$. \n",
    "\n",
    "There are two approaches\n",
    "\n",
    "* **Value Iteration**: Define the value and use dynamic programming or other techniques to solve\n",
    " * Incrementally update value function  \n",
    " \n",
    " $V_{i+1}(s) := \\max_a \\left\\{ \\sum_{s'} P_a(s,s') \\left( r_a(s,s') + \\gamma V_i(s') \\right) \\right\\}$\n",
    " \n",
    "\n",
    "* **Policy Iteration** :\n",
    " *  For each value update incrementally update policy function until convergence\n",
    " \n",
    " $V(s) := \\sum_{s'} P_{\\pi(s)} (s,s') \\left( r_{\\pi(s)} (s,s') + \\gamma V(s') \\right) $\n",
    " \n",
    " $\\pi (s):=argmax_{a}\\left\\{\\sum _{s'}P(s'\\mid s,a)\\left(R(s'\\mid s,a)+\\gamma V(s')\\right)\\right\\}$\n",
    "\n",
    "In Reinforcement Learning, things are not so easy because often we don't know the $P$ and or $r$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reinforcement Learning Approaches\n",
    "\n",
    " * **Model Based**: Learn transition model or dynamics of environment first\n",
    "\n",
    " * **Model Free** : Don't attempt to learn model or dynamics. Learn by exploration.\n",
    "  * Learn **Q function**, generalization of value function. More on this later\n",
    "  * Learn **policy** directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Balancing Exploration and Exploitation\n",
    "\n",
    " * **Greedy**: always take greedy action\t\n",
    "\n",
    " * **$\\epsilon$-greedy** : with probability ùúñ take random action otherwise take greedy action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agents\n",
    "\n",
    "\n",
    "![](support/algorithm_taxonomy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computational Intractability\n",
    "\n",
    "For Games of perfect information like Chess or GO. We know the model (dynamics of the environment).\n",
    "\n",
    "Theoretically, we should be able to compute the optimal value function.\n",
    "\n",
    "Practically, for any non-trivial game the amount moves needed to compute this is roughly $b^d$.\n",
    "\n",
    "For chess: $35^{80}$\n",
    "\n",
    "Number of particles in the universe: $10^{86}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Deep Reinforcement Learning and Function Approximation\n",
    "\n",
    "When state space is large, use Deep Neural Networks to approximate key functions in Reinforcement Learning\n",
    "\n",
    " * **Model Based**:\n",
    "  * Approximate **Value** function and/or **Policy** function with neural net\n",
    "\n",
    " * **Model Free** : \n",
    "  * Q-Learning: Approximate **Q function** with neural network\n",
    "  * Policy Optimization: Approximate **policy** with neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 'Model' Based Deep RL\n",
    "Examples:\n",
    "* AlphaGo\n",
    "* AlphaGo Zero\n",
    "* AlphaZero\n",
    "\n",
    "Combine Neural Networks for $V$ and $\\pi$ approximation with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Free Agents\n",
    "\n",
    "* **Q-Learning**: Learn Q function\n",
    " * Learn the value of taking an action from a given state.\n",
    "\t‚ÄòQ-Value‚Äô is the expected return after taking the action.\n",
    "    \n",
    "\n",
    "* **Policy Optimization**: Learn policy directly\n",
    " * Learn the action to take from a given state (i.e. observations).\n",
    "\tWe call the model for this the policy, denoted ùúã_ùúÉ (ùëé|ùë†).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Q-Learning\n",
    "\n",
    "Define Q function:\n",
    "$$ ùëÑ^{\\pi}_(s,a) = \\mathtt{E}_{\\pi} [R_t |s,a] $$\n",
    "$$ ùëÑ^‚àó(s,a) = max_{\\pi}‚Å° ùëÑ^{\\pi} (s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### DQN: Deep Q Network \n",
    "\n",
    "* Approximate Q function with Neural Net\n",
    "* Uses ‚Äòexperience replay‚Äò\n",
    "* Uses a ‚Äòtarget‚Äô network\n",
    "\n",
    "![](support/dqn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Policy Gradients\n",
    "\n",
    "Objective: $$ J(\\theta) = v_{\\pi_\\theta } (s_0)$$\n",
    "\n",
    "Gradient : $$ \\nabla J(\\theta) = R_t  \\dfrac{\\nabla_{\\pi_\\theta} (a_t‚îÇs)}{\\pi_\\theta (a_t‚îÇs)} = R_t(\\nabla log{\\pi_\\theta}(a_t‚îÇs))$$\n",
    "\n",
    "Gradient ascent: $$ \\theta_{t+1}‚Üê\\theta_t + \\alpha \\nabla J(\\theta) $$\n",
    "$$ \\theta_{t+1}‚Üê\\theta_t + \\alpha R_t(\\nabla log{\\pi_\\theta}(a_t‚îÇs)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Optimization \n",
    "![](support/policy_gradients.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PPO: Proximal Policy Optimization\n",
    "Relative Improvement:\n",
    "$$r_t (\\theta)= \\dfrac{\\pi_\\theta (a_t |s_t)}{\\pi_{\\theta_{old}} (a_t |s_t)}$$\n",
    "\n",
    "PPO Objective: \n",
    "\n",
    "$$ max_{\\theta}‚Å°\\mathtt{E}_t [min(r_t (\\theta) A_t, ùëêùëôùëñùëù(r_t (\\theta), 1‚àí\\epsilon, 1+\\epsilon) ùê¥_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://nervanasystems.github.io/coach/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Train and Deploy with SageMaker RL\n",
    "\n",
    "![](support/sagemaker_rl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-06T21:56:37.022037Z",
     "start_time": "2018-06-06T21:56:35.507701Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Portfolio Management with Amazon SageMaker RL\n",
    "\n",
    "**RL Toolkit** : Coach\n",
    "\n",
    "**DL Framework** : MXNet\n",
    "\n",
    "**RL Environment**: OpenAIGym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "We start with $m$ preselected stocks. Without loss of generality, the total investment value is set as 1 dollar at the initial timestamp. At timestamp $t$, letting $v_{m,t}$ denote the closing price of stock $m$, the *price relative vector* is defined as \n",
    "$$ y_t = ( 1, \\frac{v_{1,t}}{v_{1,t-1}}, \\frac{v_{2,t}}{v_{2,t-1}}, \\dots, \\frac{v_{m,t}}{v_{m,t-1}} ). $$\n",
    "The first element corresponds to the cash we maintain. The cash value doesn't change along time so it is always 1. During training, the investment redistribution at step $t$ is characterized by the portfolio weight vector $\\mathbf{\\omega} = (\\omega_{0,t}, \\omega_{1,t}, \\dots, \\omega_{m,t})$. \n",
    "\n",
    "1. *Objective:*\n",
    "The portfolio consists of a group of stocks. We aim to maximize the portfolio value by adjusting the weights of each stock and reallocating the portfolio at the end of each day.\n",
    "\n",
    "2. *Environment:*\n",
    "Custom developed environment using Gym.\n",
    "\n",
    "3. *States:*\n",
    "Portfolio weight vector from last trading day $\\omega_{t-1}$. Historic price tensor constructed using close, open, high, low prices of each stock. For more details, please refer to [1].\n",
    "\n",
    "4. *Actions:*\n",
    "New weight vector $\\omega_{t}$ satisfying $\\sum_{i=0}^{m}\\omega_{i,t}=1$.\n",
    "\n",
    "5. *Reward:* \n",
    "Average logarithmic cumulated return. Consider a trading cost factor $\\mu$, the average logarithmic cumulated return after timestamp $T$ is $$ R := \\frac{1}{T} \\sum_{t=1}^{T+1} \\ln(\\mu_{t}y_{t}\\cdot\\omega_{t-1}).$$\n",
    "We use the maximum rate at Poloniex and set $\\mu=0.25\\%$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "In this notebook, we use the dataset generated by [Chi Zhang](https://github.com/vermouth1992/drl-portfolio-management/tree/master/src/utils/datasets). It contains the historic price of 16 target stocks from NASDAQ100, including open, close, high and low prices from 2012-08-13 to 2017-08-11. Specifically, those stocks are: ‚ÄúAAPL‚Äù, ‚ÄúATVI‚Äù, ‚ÄúCMCSA‚Äù, ‚ÄúCOST‚Äù, ‚ÄúCSX‚Äù, ‚ÄúDISH‚Äù, ‚ÄúEA‚Äù, ‚ÄúEBAY‚Äù, ‚ÄúFB‚Äù, ‚ÄúGOOGL‚Äù, ‚ÄúHAS‚Äù, ‚ÄúILMN‚Äù, ‚ÄúINTC‚Äù, ‚ÄúMAR‚Äù, ‚ÄúREGN‚Äù and ‚ÄúSBUX‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket path: s3://sagemaker-us-east-1-412868550678/\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "sys.path.append(\"common\")\n",
    "from misc import get_execution_role, wait_for_s3_object\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "\n",
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))\n",
    "job_name_prefix = 'rl-portfolio-management'\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environment\n",
    "\n",
    "Custom environment built on Open AI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33mModified from https://github.com/vermouth1992/drl-portfolio-management/blob/master/src/environment/portfolio.py\u001b[39;49;00m\r\n",
      "\u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mgym.spaces\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m pprint\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mconfig\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mPortfolioEnv\u001b[39;49;00m(gym.Env):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    An environment for financial portfolio management.\u001b[39;49;00m\r\n",
      "\u001b[33m    Financial portfolio management is the process of constant redistribution of a fund into different\u001b[39;49;00m\r\n",
      "\u001b[33m    financial products.\u001b[39;49;00m\r\n",
      "\u001b[33m    Based on [Jiang 2017](https://arxiv.org/abs/1706.10059)\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m,\r\n",
      "                 steps=\u001b[34m730\u001b[39;49;00m,  \u001b[37m# 2 years\u001b[39;49;00m\r\n",
      "                 trading_cost=\u001b[34m0.0025\u001b[39;49;00m,\r\n",
      "                 time_cost=\u001b[34m0.00\u001b[39;49;00m,\r\n",
      "                 window_length=\u001b[34m7\u001b[39;49;00m,\r\n",
      "                 start_idx=\u001b[34m0\u001b[39;49;00m,\r\n",
      "                 sample_start_date=\u001b[36mNone\u001b[39;49;00m\r\n",
      "                 ):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        An environment for financial portfolio management.\u001b[39;49;00m\r\n",
      "\u001b[33m        Params:\u001b[39;49;00m\r\n",
      "\u001b[33m            steps - steps in episode\u001b[39;49;00m\r\n",
      "\u001b[33m            scale - scale data and each episode (except return)\u001b[39;49;00m\r\n",
      "\u001b[33m            augment - fraction to randomly shift data by\u001b[39;49;00m\r\n",
      "\u001b[33m            trading_cost - cost of trade as a fraction\u001b[39;49;00m\r\n",
      "\u001b[33m            time_cost - cost of holding as a fraction\u001b[39;49;00m\r\n",
      "\u001b[33m            window_length - how many past observations to return\u001b[39;49;00m\r\n",
      "\u001b[33m            start_idx - The number of days from '2012-08-13' of the dataset\u001b[39;49;00m\r\n",
      "\u001b[33m            sample_start_date - The start date sampling from the history\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "        datafile = DATA_DIR\r\n",
      "        history, abbreviation = read_stock_history(filepath=datafile)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.window_length = window_length\r\n",
      "        \u001b[36mself\u001b[39;49;00m.num_stocks = history.shape[\u001b[34m0\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_idx = start_idx\r\n",
      "        \u001b[36mself\u001b[39;49;00m.csv_file = CSV_DIR\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.src = DataGenerator(history=history,\r\n",
      "                                 abbreviation=abbreviation,\r\n",
      "                                 steps=steps,\r\n",
      "                                 window_length=window_length,\r\n",
      "                                 start_idx=start_idx,\r\n",
      "                                 start_date=sample_start_date)\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sim = PortfolioSim(\r\n",
      "            asset_names=abbreviation,\r\n",
      "            trading_cost=trading_cost,\r\n",
      "            time_cost=time_cost,\r\n",
      "            steps=steps)\r\n",
      "\r\n",
      "        \u001b[37m# openai gym attributes\u001b[39;49;00m\r\n",
      "        \u001b[37m# action will be the portfolio weights [cash_bias,w1,w2...] where wn are [0, 1] for each asset\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.action_space = gym.spaces.Box(\r\n",
      "            \u001b[34m0\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, shape=(\u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.src.asset_names) + \u001b[34m1\u001b[39;49;00m,), dtype=np.float32)\r\n",
      "\r\n",
      "        \u001b[37m# get the observation space from the data min and max\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=(\u001b[36mlen\u001b[39;49;00m(abbreviation), window_length,\r\n",
      "                                                                                 history.shape[-\u001b[34m1\u001b[39;49;00m] - \u001b[34m1\u001b[39;49;00m), dtype=np.float32)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mstep\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, action):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._step(action)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_step\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, action):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Step the env.\u001b[39;49;00m\r\n",
      "\u001b[33m        Actions should be portfolio [w0...]\u001b[39;49;00m\r\n",
      "\u001b[33m        - Where wn is a portfolio weight from 0 to 1. The first is cash_bias\u001b[39;49;00m\r\n",
      "\u001b[33m        - cn is the portfolio conversion weights see PortioSim._step for description\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        np.testing.assert_almost_equal(\r\n",
      "            action.shape,\r\n",
      "            (\u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.sim.asset_names) + \u001b[34m1\u001b[39;49;00m,)\r\n",
      "        )\r\n",
      "\r\n",
      "        \u001b[37m# normalise just in case\u001b[39;49;00m\r\n",
      "        weights = np.clip(action, \u001b[34m0\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)\r\n",
      "        weights /= (weights.sum() + EPS)\r\n",
      "        weights[\u001b[34m0\u001b[39;49;00m] += np.clip(\u001b[34m1\u001b[39;49;00m - weights.sum(), \u001b[34m0\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m)  \u001b[37m# so if weights are all zeros we normalise to [1,0...]\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[34massert\u001b[39;49;00m ((action >= \u001b[34m0\u001b[39;49;00m) * (action <= \u001b[34m1\u001b[39;49;00m)).all(), \u001b[33m'\u001b[39;49;00m\u001b[33mall action values should be between 0 and 1. Not \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % action\r\n",
      "        np.testing.assert_almost_equal(\r\n",
      "            np.sum(weights), \u001b[34m1.0\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, err_msg=\u001b[33m'\u001b[39;49;00m\u001b[33mweights should sum to 1. action=\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % weights)\r\n",
      "\r\n",
      "        observation, done1, ground_truth_obs = \u001b[36mself\u001b[39;49;00m.src._step()\r\n",
      "\r\n",
      "        \u001b[37m# concatenate observation with ones\u001b[39;49;00m\r\n",
      "        cash_observation = np.ones((\u001b[34m1\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.window_length, observation.shape[\u001b[34m2\u001b[39;49;00m]))\r\n",
      "        observation_concat = np.concatenate((cash_observation, observation), axis=\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        cash_ground_truth = np.ones((\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, ground_truth_obs.shape[\u001b[34m2\u001b[39;49;00m]))\r\n",
      "        ground_truth_obs = np.concatenate((cash_ground_truth, ground_truth_obs), axis=\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[37m# relative price vector of last observation day (close/open)\u001b[39;49;00m\r\n",
      "        close_price_vector = observation_concat[:, -\u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m]\r\n",
      "        open_price_vector = observation_concat[:, -\u001b[34m1\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m]\r\n",
      "        y1 = close_price_vector / open_price_vector\r\n",
      "        reward, info, done2 = \u001b[36mself\u001b[39;49;00m.sim._step(weights, y1)\r\n",
      "\r\n",
      "        \u001b[37m# calculate return for buy and hold a bit of each asset\u001b[39;49;00m\r\n",
      "        info[\u001b[33m'\u001b[39;49;00m\u001b[33mmarket_value\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = np.cumprod([inf[\u001b[33m\"\u001b[39;49;00m\u001b[33mreturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m inf \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.infos + [info]])[-\u001b[34m1\u001b[39;49;00m]\r\n",
      "        \u001b[37m# add dates\u001b[39;49;00m\r\n",
      "        info[\u001b[33m'\u001b[39;49;00m\u001b[33mdate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = index_to_date(\u001b[36mself\u001b[39;49;00m.start_idx + \u001b[36mself\u001b[39;49;00m.src.idx + \u001b[36mself\u001b[39;49;00m.src.step)\r\n",
      "        info[\u001b[33m'\u001b[39;49;00m\u001b[33msteps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = \u001b[36mself\u001b[39;49;00m.src.step\r\n",
      "        info[\u001b[33m'\u001b[39;49;00m\u001b[33mnext_obs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ground_truth_obs\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.infos.append(info)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m done1:\r\n",
      "            \u001b[37m# Save it to file\u001b[39;49;00m\r\n",
      "            keys = \u001b[36mself\u001b[39;49;00m.infos[\u001b[34m0\u001b[39;49;00m].keys()\r\n",
      "            \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.csv_file, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, newline=\u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
      "                dict_writer = csv.DictWriter(f, keys)\r\n",
      "                dict_writer.writeheader()\r\n",
      "                dict_writer.writerows(\u001b[36mself\u001b[39;49;00m.infos)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m observation, reward, done1 \u001b[35mor\u001b[39;49;00m done2, info\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._reset()\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_reset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.infos = []\r\n",
      "        \u001b[36mself\u001b[39;49;00m.sim.reset()\r\n",
      "        observation, ground_truth_obs = \u001b[36mself\u001b[39;49;00m.src.reset()\r\n",
      "        cash_observation = np.ones((\u001b[34m1\u001b[39;49;00m, \u001b[36mself\u001b[39;49;00m.window_length, observation.shape[\u001b[34m2\u001b[39;49;00m]))\r\n",
      "        observation_concat = np.concatenate((cash_observation, observation), axis=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        cash_ground_truth = np.ones((\u001b[34m1\u001b[39;49;00m, \u001b[34m1\u001b[39;49;00m, ground_truth_obs.shape[\u001b[34m2\u001b[39;49;00m]))\r\n",
      "        ground_truth_obs = np.concatenate((cash_ground_truth, ground_truth_obs), axis=\u001b[34m0\u001b[39;49;00m)\r\n",
      "        info = {}\r\n",
      "        info[\u001b[33m'\u001b[39;49;00m\u001b[33mnext_obs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ground_truth_obs\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m observation\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mDataGenerator\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"Acts as data provider for each new episode.\"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, history, abbreviation, steps=\u001b[34m730\u001b[39;49;00m, window_length=\u001b[34m50\u001b[39;49;00m, start_idx=\u001b[34m0\u001b[39;49;00m, start_date=\u001b[36mNone\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m        Args:\u001b[39;49;00m\r\n",
      "\u001b[33m            history: (num_stocks, timestamp, 5) open, high, low, close, volume\u001b[39;49;00m\r\n",
      "\u001b[33m            abbreviation: a list of length num_stocks with assets name\u001b[39;49;00m\r\n",
      "\u001b[33m            steps: the total number of steps to simulate, default is 2 years\u001b[39;49;00m\r\n",
      "\u001b[33m            window_length: observation window, must be less than 50\u001b[39;49;00m\r\n",
      "\u001b[33m            start_date: the date to start. Default is None and random pick one.\u001b[39;49;00m\r\n",
      "\u001b[33m                        It should be a string e.g. '2012-08-13'\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[34massert\u001b[39;49;00m history.shape[\u001b[34m0\u001b[39;49;00m] == \u001b[36mlen\u001b[39;49;00m(abbreviation), \u001b[33m'\u001b[39;49;00m\u001b[33mNumber of stock is not consistent\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcopy\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[36mself\u001b[39;49;00m.steps = steps + \u001b[34m1\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.window_length = window_length\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_idx = start_idx\r\n",
      "        \u001b[36mself\u001b[39;49;00m.start_date = start_date\r\n",
      "\r\n",
      "        \u001b[37m# make immutable class\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m._data = history.copy()  \u001b[37m# all data\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.asset_names = copy.copy(abbreviation)\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_step\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[37m# get price matrix (open, close, high, low) from history.\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step += \u001b[34m1\u001b[39;49;00m\r\n",
      "        obs = \u001b[36mself\u001b[39;49;00m.data[:, \u001b[36mself\u001b[39;49;00m.step:\u001b[36mself\u001b[39;49;00m.step + \u001b[36mself\u001b[39;49;00m.window_length, :].copy()\r\n",
      "\r\n",
      "        \u001b[37m# used for compute optimal action and sanity check\u001b[39;49;00m\r\n",
      "        ground_truth_obs = \u001b[36mself\u001b[39;49;00m.data[:, \u001b[36mself\u001b[39;49;00m.step + \u001b[36mself\u001b[39;49;00m.window_length:\u001b[36mself\u001b[39;49;00m.step + \u001b[36mself\u001b[39;49;00m.window_length + \u001b[34m1\u001b[39;49;00m, :].copy()\r\n",
      "\r\n",
      "        done = \u001b[36mself\u001b[39;49;00m.step >= \u001b[36mself\u001b[39;49;00m.steps\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m obs, done, ground_truth_obs\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.step = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[37m# get data for this episode, each episode might be different.\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.start_date \u001b[35mis\u001b[39;49;00m \u001b[36mNone\u001b[39;49;00m:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.idx = np.random.randint(\r\n",
      "                low=\u001b[36mself\u001b[39;49;00m.window_length, high=\u001b[36mself\u001b[39;49;00m._data.shape[\u001b[34m1\u001b[39;49;00m] - \u001b[36mself\u001b[39;49;00m.steps)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[37m# compute index corresponding to start_date for repeatable sequence\u001b[39;49;00m\r\n",
      "            \u001b[36mself\u001b[39;49;00m.idx = date_to_index(\u001b[36mself\u001b[39;49;00m.start_date) - \u001b[36mself\u001b[39;49;00m.start_idx\r\n",
      "            \u001b[34massert\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.idx >= \u001b[36mself\u001b[39;49;00m.window_length \u001b[35mand\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.idx <= \u001b[36mself\u001b[39;49;00m._data.shape[\u001b[34m1\u001b[39;49;00m] - \u001b[36mself\u001b[39;49;00m.steps, \\\r\n",
      "                \u001b[33m'\u001b[39;49;00m\u001b[33mInvalid start date, must be window_length day after start date and simulation steps day before end date\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        data = \u001b[36mself\u001b[39;49;00m._data[:, \u001b[36mself\u001b[39;49;00m.idx - \u001b[36mself\u001b[39;49;00m.window_length:\u001b[36mself\u001b[39;49;00m.idx + \u001b[36mself\u001b[39;49;00m.steps + \u001b[34m1\u001b[39;49;00m, :\u001b[34m4\u001b[39;49;00m]\r\n",
      "        \u001b[36mself\u001b[39;49;00m.data = data\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.data[:, \u001b[36mself\u001b[39;49;00m.step:\u001b[36mself\u001b[39;49;00m.step + \u001b[36mself\u001b[39;49;00m.window_length, :].copy(), \\\r\n",
      "               \u001b[36mself\u001b[39;49;00m.data[:, \u001b[36mself\u001b[39;49;00m.step + \u001b[36mself\u001b[39;49;00m.window_length:\u001b[36mself\u001b[39;49;00m.step + \u001b[36mself\u001b[39;49;00m.window_length + \u001b[34m1\u001b[39;49;00m, :].copy()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mPortfolioSim\u001b[39;49;00m(\u001b[36mobject\u001b[39;49;00m):\r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    Portfolio management sim.\u001b[39;49;00m\r\n",
      "\u001b[33m    Params:\u001b[39;49;00m\r\n",
      "\u001b[33m    - cost e.g. 0.0025 is max in Poliniex\u001b[39;49;00m\r\n",
      "\u001b[33m    Based of [Jiang 2017](https://arxiv.org/abs/1706.10059)\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, asset_names=\u001b[36mlist\u001b[39;49;00m(), steps=\u001b[34m730\u001b[39;49;00m, trading_cost=\u001b[34m0.0025\u001b[39;49;00m, time_cost=\u001b[34m0.0\u001b[39;49;00m):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.asset_names = asset_names\r\n",
      "        \u001b[36mself\u001b[39;49;00m.cost = trading_cost\r\n",
      "        \u001b[36mself\u001b[39;49;00m.time_cost = time_cost\r\n",
      "        \u001b[36mself\u001b[39;49;00m.steps = steps\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_step\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, w1, y1):\r\n",
      "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m        Step.\u001b[39;49;00m\r\n",
      "\u001b[33m        w1 - new action of portfolio weights - e.g. [0.1,0.9,0.0]\u001b[39;49;00m\r\n",
      "\u001b[33m        y1 - price relative vector also called return\u001b[39;49;00m\r\n",
      "\u001b[33m            e.g. [1.0, 0.9, 1.1]\u001b[39;49;00m\r\n",
      "\u001b[33m        Numbered equations are from https://arxiv.org/abs/1706.10059\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[34massert\u001b[39;49;00m w1.shape == y1.shape, \u001b[33m'\u001b[39;49;00m\u001b[33mw1 and y1 must have the same shape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "        \u001b[34massert\u001b[39;49;00m y1[\u001b[34m0\u001b[39;49;00m] == \u001b[34m1.0\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33my1[0] must be 1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "        w0 = \u001b[36mself\u001b[39;49;00m.w0\r\n",
      "        p0 = \u001b[36mself\u001b[39;49;00m.p0\r\n",
      "\r\n",
      "        dw1 = (y1 * w0) / (np.dot(y1, w0) + EPS)  \u001b[37m# (eq7) weights evolve into\u001b[39;49;00m\r\n",
      "        mu1 = \u001b[36mself\u001b[39;49;00m.cost * (np.abs(dw1 - w1)).sum()  \u001b[37m# (eq16) cost to change portfolio\u001b[39;49;00m\r\n",
      "        \u001b[34massert\u001b[39;49;00m mu1 < \u001b[34m1.0\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mCost is larger than current holding\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "        p1 = p0 * (\u001b[34m1\u001b[39;49;00m - mu1) * np.dot(y1, w1)  \u001b[37m# (eq11) final portfolio value\u001b[39;49;00m\r\n",
      "        p1 = p1 * (\u001b[34m1\u001b[39;49;00m - \u001b[36mself\u001b[39;49;00m.time_cost)  \u001b[37m# we can add a cost to holding\u001b[39;49;00m\r\n",
      "        p1 = np.clip(p1, \u001b[34m0\u001b[39;49;00m, np.inf)  \u001b[37m# no shorts\u001b[39;49;00m\r\n",
      "\r\n",
      "        rho1 = p1 / p0 - \u001b[34m1\u001b[39;49;00m  \u001b[37m# rate of returns\u001b[39;49;00m\r\n",
      "        r1 = np.log((p1 + EPS) / (p0 + EPS))  \u001b[37m# log rate of return\u001b[39;49;00m\r\n",
      "        reward = r1 / \u001b[36mself\u001b[39;49;00m.steps * \u001b[34m1000.\u001b[39;49;00m  \u001b[37m# (22) average logarithmic accumulated return\u001b[39;49;00m\r\n",
      "        \u001b[37m# remember for next step\u001b[39;49;00m\r\n",
      "        \u001b[36mself\u001b[39;49;00m.w0 = w1\r\n",
      "        \u001b[36mself\u001b[39;49;00m.p0 = p1\r\n",
      "\r\n",
      "        \u001b[37m# if we run out of money, we're done (losing all the money)\u001b[39;49;00m\r\n",
      "        done = \u001b[36mbool\u001b[39;49;00m(p1 == \u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        info = {\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: reward,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mlog_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: r1,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mportfolio_value\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: p1,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mreturn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: y1.mean(),\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mrate_of_return\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: rho1,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mweights_mean\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: w1.mean(),\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mweights_std\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: w1.std(),\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mcost\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: mu1,\r\n",
      "        }\r\n",
      "        \u001b[36mself\u001b[39;49;00m.infos.append(info)\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m reward, info, done\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[36mself\u001b[39;49;00m.infos = []\r\n",
      "        \u001b[36mself\u001b[39;49;00m.w0 = np.array([\u001b[34m1.0\u001b[39;49;00m] + [\u001b[34m0.0\u001b[39;49;00m] * \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.asset_names))\r\n",
      "        \u001b[36mself\u001b[39;49;00m.p0 = \u001b[34m1.0\u001b[39;49;00m\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/portfolio_env.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agent and Algorithm\n",
    "\n",
    "Clipped PPO Agent with Coach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.agents.clipped_ppo_agent\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ClippedPPOAgentParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.architectures.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Conv2d\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m VisualizationParameters, PresetValidationParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.base_parameters\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MiddlewareScheme, DistributedCoachSynchronizationType\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.core_types\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainingSteps, EnvironmentEpisodes, EnvironmentSteps, RunPhase\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.environments.gym_environment\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m GymVectorEnvironment, ObservationSpaceType\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.exploration_policies.e_greedy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m EGreedyParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.basic_rl_graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m BasicRLGraphManager\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.graph_managers.graph_manager\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ScheduleParameters\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mrl_coach.schedules\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LinearSchedule\r\n",
      "\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\u001b[37m# Graph Scheduling #\u001b[39;49;00m\r\n",
      "\u001b[37m####################\u001b[39;49;00m\r\n",
      "\r\n",
      "schedule_params = ScheduleParameters()\r\n",
      "schedule_params.improve_steps = TrainingSteps(\u001b[34m60000\u001b[39;49;00m)\r\n",
      "schedule_params.steps_between_evaluation_periods = EnvironmentSteps(\u001b[34m2048\u001b[39;49;00m)\r\n",
      "schedule_params.evaluation_steps = EnvironmentEpisodes(\u001b[34m5\u001b[39;49;00m)\r\n",
      "schedule_params.heatup_steps = EnvironmentSteps(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\u001b[37m# Agent #\u001b[39;49;00m\r\n",
      "\u001b[37m#########\u001b[39;49;00m\r\n",
      "\r\n",
      "agent_params = ClippedPPOAgentParameters()\r\n",
      "\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].learning_rate = \u001b[34m0.0001\u001b[39;49;00m\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].input_embedders_parameters[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].scheme = [Conv2d(\u001b[34m32\u001b[39;49;00m, [\u001b[34m1\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m], \u001b[34m1\u001b[39;49;00m)]\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].middleware_parameters.scheme = MiddlewareScheme.Empty\r\n",
      "agent_params.network_wrappers[\u001b[33m'\u001b[39;49;00m\u001b[33mmain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].batch_size = \u001b[34m64\u001b[39;49;00m\r\n",
      "agent_params.algorithm.clipping_decay_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0\u001b[39;49;00m, \u001b[34m150000\u001b[39;49;00m)\r\n",
      "agent_params.algorithm.discount = \u001b[34m0.99\u001b[39;49;00m\r\n",
      "agent_params.algorithm.num_steps_between_copying_online_weights_to_target = EnvironmentSteps(\u001b[34m2048\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m# Distributed Coach synchronization type.\u001b[39;49;00m\r\n",
      "agent_params.algorithm.distributed_coach_synchronization_type = DistributedCoachSynchronizationType.SYNC\r\n",
      "\r\n",
      "agent_params.exploration = EGreedyParameters()\r\n",
      "agent_params.exploration.epsilon_schedule = LinearSchedule(\u001b[34m1.0\u001b[39;49;00m, \u001b[34m0.01\u001b[39;49;00m, \u001b[34m10000\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[37m###############\u001b[39;49;00m\r\n",
      "\u001b[37m# Environment #\u001b[39;49;00m\r\n",
      "\u001b[37m###############\u001b[39;49;00m\r\n",
      "\r\n",
      "env_params = GymVectorEnvironment(level=\u001b[33m'\u001b[39;49;00m\u001b[33mportfolio_env:PortfolioEnv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "env_params.\u001b[31m__dict__\u001b[39;49;00m[\u001b[33m'\u001b[39;49;00m\u001b[33mobservation_space_type\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = ObservationSpaceType.Tensor\r\n",
      "\r\n",
      "\u001b[37m########\u001b[39;49;00m\r\n",
      "\u001b[37m# Test #\u001b[39;49;00m\r\n",
      "\u001b[37m########\u001b[39;49;00m\r\n",
      "\r\n",
      "preset_validation_params = PresetValidationParameters()\r\n",
      "preset_validation_params.test = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "\r\n",
      "graph_manager = BasicRLGraphManager(agent_params=agent_params, env_params=env_params,\r\n",
      "                                    schedule_params=schedule_params, vis_params=VisualizationParameters(),\r\n",
      "                                    preset_validation_params=preset_validation_params)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/preset-portfolio-management-clippedppo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Code\n",
    "\n",
    "SageMakerRL prebuilt functions to training coach agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_rl.coach_launcher\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SageMakerCoachPresetLauncher\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mMyLauncher\u001b[39;49;00m(SageMakerCoachPresetLauncher):\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mdefault_preset_name\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\r\n",
      "        \u001b[33m\"\"\"This points to a .py file that configures everything about the RL job.\u001b[39;49;00m\r\n",
      "\u001b[33m        It can be overridden at runtime by specifying the RLCOACH_PRESET hyperparameter.\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mpreset-portfolio-management-clippedppo\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mmap_hyperparameter\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, name, value):\r\n",
      "        \u001b[33m\"\"\"Here we configure some shortcut names for hyperparameters that we expect to use frequently.\u001b[39;49;00m\r\n",
      "\u001b[33m        Essentially anything in the preset file can be overridden through a hyperparameter with a name\u001b[39;49;00m\r\n",
      "\u001b[33m        like \"rl.agent_params.algorithm.etc\".\u001b[39;49;00m\r\n",
      "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
      "        \u001b[37m# maps from alias (key) to fully qualified coach parameter (value)\u001b[39;49;00m\r\n",
      "        mapping = {\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mdiscount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.agent_params.algorithm.discount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mevaluation_episodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.evaluation_steps:EnvironmentEpisodes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\r\n",
      "            \u001b[33m\"\u001b[39;49;00m\u001b[33mimprove_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: \u001b[33m\"\u001b[39;49;00m\u001b[33mrl.improve_steps:TrainingSteps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\r\n",
      "        }\r\n",
      "        \u001b[34mif\u001b[39;49;00m name \u001b[35min\u001b[39;49;00m mapping:\r\n",
      "            \u001b[36mself\u001b[39;49;00m.apply_hyperparameter(mapping[name], value)\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            \u001b[36msuper\u001b[39;49;00m().map_hyperparameter(name, value)\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    MyLauncher.train_main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train-coach.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model Training\n",
    "Create SageMaker RL Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-12 21:36:04 Starting - Starting the training job...\n",
      "2019-06-12 21:36:05 Starting - Launching requested ML instances......\n",
      "2019-06-12 21:37:14 Starting - Preparing the instances for training......\n",
      "2019-06-12 21:38:35 Downloading - Downloading input data\n",
      "2019-06-12 21:38:35 Training - Training image download completed. Training in progress..\n",
      "\u001b[31mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[31mbash: no job control in this shell\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:36,849 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:36,852 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:36,867 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_MODEL_DIR': '/opt/ml/model', 'SM_HPS': '{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5}', 'SM_NUM_GPUS': '0', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_estimator\":\"RLEstimator\"}', 'SM_HP_RLCOACH_PRESET': 'preset-portfolio-management-clippedppo', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"rl-portfolio-management-2019-06-12-21-36-03-778\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/source/sourcedir.tar.gz\",\"module_name\":\"train-coach\",\"network_interface_name\":\"ethwe\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train-coach.py\"}', 'SM_MODULE_NAME': 'train-coach', 'SM_HP_RL.AGENT_PARAMS.ALGORITHM.DISCOUNT': '0.9', 'SM_USER_ARGS': '[\"--RLCOACH_PRESET\",\"preset-portfolio-management-clippedppo\",\"--rl.agent_params.algorithm.discount\",\"0.9\",\"--rl.evaluation_steps:EnvironmentEpisodes\",\"5\"]', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_LOG_LEVEL': '20', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[]', 'SM_NUM_CPUS': '16', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_HOSTS': '[\"algo-1\"]', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_NETWORK_INTERFACE_NAME': 'ethwe', 'SM_HP_RL.EVALUATION_STEPS:ENVIRONMENTEPISODES': '5', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_USER_ENTRY_POINT': 'train-coach.py', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_DIR': 's3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/source/sourcedir.tar.gz'}\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:37,012 sagemaker-containers INFO     Module train-coach does not provide a setup.py. \u001b[0m\n",
      "\u001b[31mGenerating setup.py\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:37,013 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:37,013 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:37,013 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m pip install -U . \u001b[0m\n",
      "\u001b[31mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[31mBuilding wheels for collected packages: train-coach\n",
      "  Running setup.py bdist_wheel for train-coach: started\u001b[0m\n",
      "\u001b[31m  Running setup.py bdist_wheel for train-coach: finished with status 'done'\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vw1nopnh/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\u001b[0m\n",
      "\u001b[31mSuccessfully built train-coach\u001b[0m\n",
      "\u001b[31mInstalling collected packages: train-coach\u001b[0m\n",
      "\u001b[31mSuccessfully installed train-coach-1.0.0\u001b[0m\n",
      "\u001b[31mYou are using pip version 18.1, however version 19.1.1 is available.\u001b[0m\n",
      "\u001b[31mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:38,791 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[31m2019-06-12 21:38:38,805 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[31mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[31m{\n",
      "    \"num_cpus\": 16,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"network_interface_name\": \"ethwe\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/source/sourcedir.tar.gz\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"rl.agent_params.algorithm.discount\": 0.9,\n",
      "        \"RLCOACH_PRESET\": \"preset-portfolio-management-clippedppo\",\n",
      "        \"rl.evaluation_steps:EnvironmentEpisodes\": 5\n",
      "    },\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_estimator\": \"RLEstimator\"\n",
      "    },\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"log_level\": 20,\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"input_data_config\": {},\n",
      "    \"job_name\": \"rl-portfolio-management-2019-06-12-21-36-03-778\",\n",
      "    \"user_entry_point\": \"train-coach.py\",\n",
      "    \"module_name\": \"train-coach\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"num_gpus\": 0,\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"ethwe\"\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\"\u001b[0m\n",
      "\u001b[31m}\n",
      "\u001b[0m\n",
      "\u001b[31mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[31mSM_MODULE_NAME=train-coach\u001b[0m\n",
      "\u001b[31mSM_HPS={\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5}\u001b[0m\n",
      "\u001b[31mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_PARAMS={\"sagemaker_estimator\":\"RLEstimator\"}\u001b[0m\n",
      "\u001b[31mSM_HP_RLCOACH_PRESET=preset-portfolio-management-clippedppo\u001b[0m\n",
      "\u001b[31mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_estimator\":\"RLEstimator\"},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"RLCOACH_PRESET\":\"preset-portfolio-management-clippedppo\",\"rl.agent_params.algorithm.discount\":0.9,\"rl.evaluation_steps:EnvironmentEpisodes\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"rl-portfolio-management-2019-06-12-21-36-03-778\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/source/sourcedir.tar.gz\",\"module_name\":\"train-coach\",\"network_interface_name\":\"ethwe\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"},\"user_entry_point\":\"train-coach.py\"}\u001b[0m\n",
      "\u001b[31mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[31mSM_HP_RL.AGENT_PARAMS.ALGORITHM.DISCOUNT=0.9\u001b[0m\n",
      "\u001b[31mSM_USER_ARGS=[\"--RLCOACH_PRESET\",\"preset-portfolio-management-clippedppo\",\"--rl.agent_params.algorithm.discount\",\"0.9\",\"--rl.evaluation_steps:EnvironmentEpisodes\",\"5\"]\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[31mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[31mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\u001b[0m\n",
      "\u001b[31mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"ethwe\"}\u001b[0m\n",
      "\u001b[31mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[31mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[31mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[31mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[31mSM_USER_ENTRY_POINT=train-coach.py\u001b[0m\n",
      "\u001b[31mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[31mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[31mSM_NETWORK_INTERFACE_NAME=ethwe\u001b[0m\n",
      "\u001b[31mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[31mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[31mSM_HP_RL.EVALUATION_STEPS:ENVIRONMENTEPISODES=5\u001b[0m\n",
      "\u001b[31mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[31mSM_MODULE_DIR=s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/source/sourcedir.tar.gz\n",
      "\u001b[0m\n",
      "\u001b[31mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[31m/usr/bin/python -m train-coach --RLCOACH_PRESET preset-portfolio-management-clippedppo --rl.agent_params.algorithm.discount 0.9 --rl.evaluation_steps:EnvironmentEpisodes 5\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m#033[93mWarning: failed to import the following packages - tensorflow#033[0m\u001b[0m\n",
      "\u001b[31mApplying RL hyperparameter rl.agent_params.algorithm.discount=0.9\u001b[0m\n",
      "\u001b[31mApplying RL hyperparameter rl.evaluation_steps:EnvironmentEpisodes=5\u001b[0m\n",
      "\u001b[31mLoading preset preset-portfolio-management-clippedppo from /opt/ml/code\u001b[0m\n",
      "\u001b[31m## Creating graph - name: BasicRLGraphManager\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\u001b[0m\n",
      "\u001b[31m## Creating agent - name: agent\u001b[0m\n",
      "\u001b[31mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[31mRequested devices [gpu(0)] not available. Default to CPU context.\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/mxnet/gluon/block.py:303: UserWarning: \"PPOHead._loss\" is an unregistered container with Blocks. Note that Blocks inside the list, tuple or dict will not be registered automatically. Make sure to register them using register_child() or switching to nn.Sequential/nn.HybridSequential instead. \n",
      "  ret.update(cld.collect_params(select=select))\u001b[0m\n",
      "\u001b[31m## Starting to improve simple_rl_graph task index 0\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=1, Total reward=-1.6, Steps=731, Training iteration=0\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=2, Total reward=-1.63, Steps=1462, Training iteration=0\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=2, Total reward=-3.11, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=2, Total reward=-3.45, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=2, Total reward=-3.35, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=2, Total reward=-3.04, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=2, Total reward=-3.35, Steps=2048, Training iteration=0\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -3.26\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/0_Step-2049.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=3, Total reward=-2.04, Steps=2779, Training iteration=0\u001b[0m\n",
      "\u001b[31m/usr/local/lib/python3.5/dist-packages/rl_coach/architectures/mxnet_components/heads/head.py:95: UserWarning: Parameter clippedppolosscontinuous0_kl_coefficient is not used by any computation. Is this intended?\n",
      "  outputs = super(HeadLoss, self).forward(*args)\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.011369742453098297, KL divergence=[0.], Entropy=[-0.2412446], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.042911920696496964, KL divergence=[0.], Entropy=[-0.24129401], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.04591741785407066, KL divergence=[0.], Entropy=[-0.24133375], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.05583664029836655, KL divergence=[0.], Entropy=[-0.24136254], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.05628142133355141, KL divergence=[0.], Entropy=[-0.2413969], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.05722259357571602, KL divergence=[0.], Entropy=[-0.24143589], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.05189281702041626, KL divergence=[0.], Entropy=[-0.24150014], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.06456755101680756, KL divergence=[0.], Entropy=[-0.24152759], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.06572694331407547, KL divergence=[0.], Entropy=[-0.24154143], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.06775469332933426, KL divergence=[0.], Entropy=[-0.24156532], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/1_Step-3496.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=4, Total reward=-1.97, Steps=3510, Training iteration=1\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-1.79, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-2.03, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-1.75, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-1.61, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=4, Total reward=-1.45, Steps=4096, Training iteration=1\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -1.72\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/2_Step-4097.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=5, Total reward=-2.2, Steps=4827, Training iteration=1\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.00026043978868983686, KL divergence=[0.], Entropy=[-0.24155597], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.0025854434352368116, KL divergence=[0.], Entropy=[-0.24151988], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.01586497575044632, KL divergence=[0.], Entropy=[-0.24147654], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.006605887785553932, KL divergence=[0.], Entropy=[-0.24142514], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.015821289271116257, KL divergence=[0.], Entropy=[-0.24138492], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.009343555197119713, KL divergence=[0.], Entropy=[-0.24135488], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.01575000397861004, KL divergence=[0.], Entropy=[-0.24132404], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.020238712430000305, KL divergence=[0.], Entropy=[-0.24126759], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.009096860885620117, KL divergence=[0.], Entropy=[-0.2412178], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.001647859695367515, KL divergence=[0.], Entropy=[-0.24119577], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=6, Total reward=-2.5, Steps=5558, Training iteration=2\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=6, Total reward=-0.82, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=6, Total reward=-0.92, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=6, Total reward=-1.11, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=6, Total reward=-1.66, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=6, Total reward=-1.13, Steps=6144, Training iteration=2\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -1.13\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/3_Step-6145.ckpt.main_level.agent.main.online']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=7, Total reward=-3.09, Steps=6875, Training iteration=2\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.012804467231035233, KL divergence=[0.], Entropy=[-0.24119559], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.009861736558377743, KL divergence=[0.], Entropy=[-0.24115314], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-4.4948676077183336e-05, KL divergence=[0.], Entropy=[-0.24111661], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.014691334217786789, KL divergence=[0.], Entropy=[-0.24107386], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.0104815773665905, KL divergence=[0.], Entropy=[-0.2410608], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.011535034514963627, KL divergence=[0.], Entropy=[-0.24104196], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.014787916094064713, KL divergence=[0.], Entropy=[-0.24100281], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.0022985993418842554, KL divergence=[0.], Entropy=[-0.24096774], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.017764516174793243, KL divergence=[0.], Entropy=[-0.24096099], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.02463098056614399, KL divergence=[0.], Entropy=[-0.24095191], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=8, Total reward=-2.37, Steps=7606, Training iteration=3\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.67, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.63, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.98, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.82, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=8, Total reward=-0.79, Steps=8192, Training iteration=3\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.78\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/4_Step-8193.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=9, Total reward=-2.47, Steps=8923, Training iteration=3\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04440062865614891, KL divergence=[0.], Entropy=[-0.24090964], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017127402126789093, KL divergence=[0.], Entropy=[-0.24090557], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.003560140496119857, KL divergence=[0.], Entropy=[-0.24091758], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.014923030510544777, KL divergence=[0.], Entropy=[-0.24091935], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0065477159805595875, KL divergence=[0.], Entropy=[-0.24090661], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.015583232045173645, KL divergence=[0.], Entropy=[-0.24092942], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.003095402615144849, KL divergence=[0.], Entropy=[-0.24093194], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.002474904526025057, KL divergence=[0.], Entropy=[-0.24092169], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.011383285745978355, KL divergence=[0.], Entropy=[-0.24092366], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.010773545131087303, KL divergence=[0.], Entropy=[-0.24093395], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=10, Total reward=-3.03, Steps=9654, Training iteration=4\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=10, Total reward=-1.02, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=10, Total reward=-1.06, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=10, Total reward=-0.68, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=10, Total reward=-0.77, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=10, Total reward=-0.58, Steps=10240, Training iteration=4\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.82\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/5_Step-10241.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=11, Total reward=-3.15, Steps=10971, Training iteration=4\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.05001787096261978, KL divergence=[0.], Entropy=[-0.24092636], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.012902343645691872, KL divergence=[0.], Entropy=[-0.24093491], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.015021744184195995, KL divergence=[0.], Entropy=[-0.24096723], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.020626747980713844, KL divergence=[0.], Entropy=[-0.24098028], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01585053652524948, KL divergence=[0.], Entropy=[-0.24099593], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.024529248476028442, KL divergence=[0.], Entropy=[-0.24101287], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.038407422602176666, KL divergence=[0.], Entropy=[-0.24105296], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.026874028146266937, KL divergence=[0.], Entropy=[-0.24108316], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.00978345237672329, KL divergence=[0.], Entropy=[-0.24111773], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.012022089213132858, KL divergence=[0.], Entropy=[-0.24113092], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=12, Total reward=-2.59, Steps=11702, Training iteration=5\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.7, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.61, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.66, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.37, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=12, Total reward=-0.94, Steps=12288, Training iteration=5\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.66\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/6_Step-12289.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=13, Total reward=-2.94, Steps=13019, Training iteration=5\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030030446127057076, KL divergence=[0.], Entropy=[-0.24116598], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01948378048837185, KL divergence=[0.], Entropy=[-0.2411959], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01619126833975315, KL divergence=[0.], Entropy=[-0.24122769], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03215300291776657, KL divergence=[0.], Entropy=[-0.24126095], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01441236212849617, KL divergence=[0.], Entropy=[-0.24129783], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01595967635512352, KL divergence=[0.], Entropy=[-0.24133517], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.00689092418178916, KL divergence=[0.], Entropy=[-0.24136847], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.009454830549657345, KL divergence=[0.], Entropy=[-0.24140772], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0015160180628299713, KL divergence=[0.], Entropy=[-0.24144693], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.010138895362615585, KL divergence=[0.], Entropy=[-0.24148163], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=14, Total reward=-2.97, Steps=13750, Training iteration=6\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=14, Total reward=0.1, Steps=14336, Training iteration=6\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=14, Total reward=-0.23, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=14, Total reward=-0.36, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=14, Total reward=-0.26, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=14, Total reward=-0.8, Steps=14336, Training iteration=6\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.31\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/7_Step-14337.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=15, Total reward=-3.18, Steps=15067, Training iteration=6\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03454485535621643, KL divergence=[0.], Entropy=[-0.24149895], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.024196533486247063, KL divergence=[0.], Entropy=[-0.24148323], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.018698345869779587, KL divergence=[0.], Entropy=[-0.24145937], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017599662765860558, KL divergence=[0.], Entropy=[-0.24145295], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.009500968270003796, KL divergence=[0.], Entropy=[-0.24145165], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.013685047626495361, KL divergence=[0.], Entropy=[-0.24145369], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02722140960395336, KL divergence=[0.], Entropy=[-0.24143738], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.020451227203011513, KL divergence=[0.], Entropy=[-0.24142252], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.020359313115477562, KL divergence=[0.], Entropy=[-0.24141397], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.012990512885153294, KL divergence=[0.], Entropy=[-0.24139729], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=16, Total reward=-2.93, Steps=15798, Training iteration=7\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.36, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.74, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.76, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.34, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=16, Total reward=-0.12, Steps=16384, Training iteration=7\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.46\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/8_Step-16385.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=17, Total reward=-2.7, Steps=17115, Training iteration=7\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03660091012716293, KL divergence=[0.], Entropy=[-0.24140173], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.008482114411890507, KL divergence=[0.], Entropy=[-0.24140772], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.013161111623048782, KL divergence=[0.], Entropy=[-0.24141926], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0021517276763916016, KL divergence=[0.], Entropy=[-0.24143511], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0011098835384473205, KL divergence=[0.], Entropy=[-0.24144617], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.011789084412157536, KL divergence=[0.], Entropy=[-0.24146047], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.004975466523319483, KL divergence=[0.], Entropy=[-0.241473], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.005035358481109142, KL divergence=[0.], Entropy=[-0.24149899], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.00249957456253469, KL divergence=[0.], Entropy=[-0.24151397], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=-0.015167337842285633, KL divergence=[0.], Entropy=[-0.24150424], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=18, Total reward=-3.24, Steps=17846, Training iteration=8\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=18, Total reward=-0.33, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=18, Total reward=-0.83, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=18, Total reward=-0.47, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=18, Total reward=-0.68, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=18, Total reward=-0.47, Steps=18432, Training iteration=8\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.55\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/9_Step-18433.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=19, Total reward=-2.74, Steps=19163, Training iteration=8\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.021065138280391693, KL divergence=[0.], Entropy=[-0.24149136], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.020744038745760918, KL divergence=[0.], Entropy=[-0.24149626], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02107400819659233, KL divergence=[0.], Entropy=[-0.2415123], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03030375950038433, KL divergence=[0.], Entropy=[-0.24152522], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.015360758639872074, KL divergence=[0.], Entropy=[-0.24153882], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0250752791762352, KL divergence=[0.], Entropy=[-0.24154535], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.013017883524298668, KL divergence=[0.], Entropy=[-0.2415604], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.006284821312874556, KL divergence=[0.], Entropy=[-0.24157767], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0035742863547056913, KL divergence=[0.], Entropy=[-0.2416026], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.021810613572597504, KL divergence=[0.], Entropy=[-0.24163073], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=20, Total reward=-2.86, Steps=19894, Training iteration=9\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=0.21, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=-0.41, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=-0.42, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=-0.28, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=20, Total reward=-0.94, Steps=20480, Training iteration=9\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.37\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/10_Step-20481.ckpt.main_level.agent.main.online']\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=21, Total reward=-2.52, Steps=21211, Training iteration=9\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.027602344751358032, KL divergence=[0.], Entropy=[-0.24163772], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.015934176743030548, KL divergence=[0.], Entropy=[-0.24164781], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02160494402050972, KL divergence=[0.], Entropy=[-0.24166478], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.032186370342969894, KL divergence=[0.], Entropy=[-0.24170828], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.018780214712023735, KL divergence=[0.], Entropy=[-0.24175169], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01406117994338274, KL divergence=[0.], Entropy=[-0.24176231], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03400639072060585, KL divergence=[0.], Entropy=[-0.24177094], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.008848460391163826, KL divergence=[0.], Entropy=[-0.24179338], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.013436385430395603, KL divergence=[0.], Entropy=[-0.24181423], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.00895741768181324, KL divergence=[0.], Entropy=[-0.2418181], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=22, Total reward=-2.78, Steps=21942, Training iteration=10\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=22, Total reward=-0.55, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=22, Total reward=-0.2, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=22, Total reward=0.01, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=22, Total reward=-0.53, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=22, Total reward=-0.11, Steps=22528, Training iteration=10\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.28\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/11_Step-22529.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=23, Total reward=-2.82, Steps=23259, Training iteration=10\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03721335902810097, KL divergence=[0.], Entropy=[-0.24182129], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01181043405085802, KL divergence=[0.], Entropy=[-0.24181837], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02972482517361641, KL divergence=[0.], Entropy=[-0.24180388], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03292912244796753, KL divergence=[0.], Entropy=[-0.24178132], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.026322834193706512, KL divergence=[0.], Entropy=[-0.24177308], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02262629196047783, KL divergence=[0.], Entropy=[-0.24175836], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0024536431301385164, KL divergence=[0.], Entropy=[-0.24175607], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.012947204522788525, KL divergence=[0.], Entropy=[-0.24175443], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.004809896927326918, KL divergence=[0.], Entropy=[-0.24175282], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0073362067341804504, KL divergence=[0.], Entropy=[-0.24174894], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=24, Total reward=-2.6, Steps=23990, Training iteration=11\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.33, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.52, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.18, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.14, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=24, Total reward=-0.81, Steps=24576, Training iteration=11\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.4\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/12_Step-24577.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=25, Total reward=-2.64, Steps=25307, Training iteration=11\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.038483429700136185, KL divergence=[0.], Entropy=[-0.24173899], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03449413925409317, KL divergence=[0.], Entropy=[-0.24173571], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03611677512526512, KL divergence=[0.], Entropy=[-0.24173708], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.021465610712766647, KL divergence=[0.], Entropy=[-0.24174456], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.027665771543979645, KL divergence=[0.], Entropy=[-0.24174614], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04050621762871742, KL divergence=[0.], Entropy=[-0.2417512], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03157748281955719, KL divergence=[0.], Entropy=[-0.24175783], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03337818756699562, KL divergence=[0.], Entropy=[-0.24175256], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.019754724577069283, KL divergence=[0.], Entropy=[-0.24175958], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02191050536930561, KL divergence=[0.], Entropy=[-0.24175173], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=26, Total reward=-2.9, Steps=26038, Training iteration=12\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=26, Total reward=-0.92, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=26, Total reward=-0.12, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=26, Total reward=-0.9, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=26, Total reward=-0.41, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=26, Total reward=-0.61, Steps=26624, Training iteration=12\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.59\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/13_Step-26625.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=27, Total reward=-2.48, Steps=27355, Training iteration=12\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03260877728462219, KL divergence=[0.], Entropy=[-0.24175414], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03030201978981495, KL divergence=[0.], Entropy=[-0.24178004], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.026439042761921883, KL divergence=[0.], Entropy=[-0.24180485], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01200015190988779, KL divergence=[0.], Entropy=[-0.24181725], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01937258057296276, KL divergence=[0.], Entropy=[-0.24183129], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028292439877986908, KL divergence=[0.], Entropy=[-0.24184908], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.033315666019916534, KL divergence=[0.], Entropy=[-0.24186659], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.010782708413898945, KL divergence=[0.], Entropy=[-0.24190544], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03292714059352875, KL divergence=[0.], Entropy=[-0.24193935], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01922958344221115, KL divergence=[0.], Entropy=[-0.24197525], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=28, Total reward=-3.14, Steps=28086, Training iteration=13\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=-0.47, Steps=28672, Training iteration=13\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=-0.66, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=-0.88, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=0.05, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=28, Total reward=-0.24, Steps=28672, Training iteration=13\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.44\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/14_Step-28673.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=29, Total reward=-3.11, Steps=29403, Training iteration=13\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04617445170879364, KL divergence=[0.], Entropy=[-0.24201241], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04286491125822067, KL divergence=[0.], Entropy=[-0.24204049], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.027604181319475174, KL divergence=[0.], Entropy=[-0.24205591], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.010408755391836166, KL divergence=[0.], Entropy=[-0.2420885], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.015632348135113716, KL divergence=[0.], Entropy=[-0.24210858], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01540732104331255, KL divergence=[0.], Entropy=[-0.24212404], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01762796752154827, KL divergence=[0.], Entropy=[-0.2421353], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.022114736959338188, KL divergence=[0.], Entropy=[-0.24215811], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017854923382401466, KL divergence=[0.], Entropy=[-0.24218422], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02374752052128315, KL divergence=[0.], Entropy=[-0.24221487], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=30, Total reward=-2.73, Steps=30134, Training iteration=14\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/15_Step-30569.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=30, Total reward=-0.73, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=30, Total reward=-0.28, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=30, Total reward=-0.32, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=30, Total reward=-0.85, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=30, Total reward=-0.25, Steps=30720, Training iteration=14\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.49\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/16_Step-30721.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=31, Total reward=-2.95, Steps=31451, Training iteration=14\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03718654438853264, KL divergence=[0.], Entropy=[-0.24222125], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.031329717487096786, KL divergence=[0.], Entropy=[-0.24223934], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.021826215088367462, KL divergence=[0.], Entropy=[-0.24225277], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02204597368836403, KL divergence=[0.], Entropy=[-0.24225664], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0175138171762228, KL divergence=[0.], Entropy=[-0.2422571], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017160721123218536, KL divergence=[0.], Entropy=[-0.24226674], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01776304841041565, KL divergence=[0.], Entropy=[-0.24226926], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028585487976670265, KL divergence=[0.], Entropy=[-0.24227251], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.027281712740659714, KL divergence=[0.], Entropy=[-0.2422781], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.024092622101306915, KL divergence=[0.], Entropy=[-0.24227947], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=32, Total reward=-3.04, Steps=32182, Training iteration=15\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=-0.02, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=-0.48, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=-0.36, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=-0.09, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=32, Total reward=-0.14, Steps=32768, Training iteration=15\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.22\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/17_Step-32769.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=33, Total reward=-3.29, Steps=33499, Training iteration=15\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03379527106881142, KL divergence=[0.], Entropy=[-0.24229535], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03478504717350006, KL divergence=[0.], Entropy=[-0.24232], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03679564222693443, KL divergence=[0.], Entropy=[-0.2423202], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017771290615200996, KL divergence=[0.], Entropy=[-0.24233085], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.040723808109760284, KL divergence=[0.], Entropy=[-0.24234962], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.014397465623915195, KL divergence=[0.], Entropy=[-0.24236217], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.016916902735829353, KL divergence=[0.], Entropy=[-0.2423591], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03938184678554535, KL divergence=[0.], Entropy=[-0.24237126], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.026280511170625687, KL divergence=[0.], Entropy=[-0.242407], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.012303567491471767, KL divergence=[0.], Entropy=[-0.24241534], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=34, Total reward=-3.36, Steps=34230, Training iteration=16\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=34, Total reward=-0.33, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=34, Total reward=-0.14, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=34, Total reward=-0.17, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=34, Total reward=-0.36, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=34, Total reward=-0.49, Steps=34816, Training iteration=16\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.3\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/18_Step-34817.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=35, Total reward=-2.53, Steps=35547, Training iteration=16\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028111612424254417, KL divergence=[0.], Entropy=[-0.24241805], training epoch=0, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPolicy training> Surrogate loss=0.02929091267287731, KL divergence=[0.], Entropy=[-0.24243107], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0258175116032362, KL divergence=[0.], Entropy=[-0.242452], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03124210424721241, KL divergence=[0.], Entropy=[-0.24247576], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02461860328912735, KL divergence=[0.], Entropy=[-0.24249396], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01864749938249588, KL divergence=[0.], Entropy=[-0.24249868], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.019830407574772835, KL divergence=[0.], Entropy=[-0.24250741], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02987232804298401, KL divergence=[0.], Entropy=[-0.24253355], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02735915593802929, KL divergence=[0.], Entropy=[-0.24256445], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02063242346048355, KL divergence=[0.], Entropy=[-0.24258518], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=36, Total reward=-3.33, Steps=36278, Training iteration=17\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=-0.57, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=-0.39, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=-0.27, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=-0.37, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=36, Total reward=-0.16, Steps=36864, Training iteration=17\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.35\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/19_Step-36865.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=37, Total reward=-2.94, Steps=37595, Training iteration=17\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.024825451895594597, KL divergence=[0.], Entropy=[-0.24260546], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04923397675156593, KL divergence=[0.], Entropy=[-0.24265505], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.027922987937927246, KL divergence=[0.], Entropy=[-0.24269623], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.041253525763750076, KL divergence=[0.], Entropy=[-0.2427227], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03886695206165314, KL divergence=[0.], Entropy=[-0.24273524], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030751051381230354, KL divergence=[0.], Entropy=[-0.24274735], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02403402328491211, KL divergence=[0.], Entropy=[-0.24276061], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.014573589898645878, KL divergence=[0.], Entropy=[-0.24277334], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.014198962599039078, KL divergence=[0.], Entropy=[-0.2427938], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03182353079319, KL divergence=[0.], Entropy=[-0.24281989], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=38, Total reward=-2.69, Steps=38326, Training iteration=18\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=38, Total reward=-0.24, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=38, Total reward=-0.44, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=38, Total reward=-0.21, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=38, Total reward=-0.2, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=38, Total reward=-0.19, Steps=38912, Training iteration=18\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.25\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/20_Step-38913.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=39, Total reward=-3.05, Steps=39643, Training iteration=18\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.043357327580451965, KL divergence=[0.], Entropy=[-0.24284929], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030399970710277557, KL divergence=[0.], Entropy=[-0.24286205], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.025257164612412453, KL divergence=[0.], Entropy=[-0.24287748], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02219696156680584, KL divergence=[0.], Entropy=[-0.24289612], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.018849484622478485, KL divergence=[0.], Entropy=[-0.24291755], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03254416212439537, KL divergence=[0.], Entropy=[-0.24292058], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.033561233431100845, KL divergence=[0.], Entropy=[-0.24293251], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.007567383348941803, KL divergence=[0.], Entropy=[-0.24296278], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0209713876247406, KL divergence=[0.], Entropy=[-0.24297957], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01699201762676239, KL divergence=[0.], Entropy=[-0.24299145], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=40, Total reward=-2.79, Steps=40374, Training iteration=19\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=-0.26, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=-0.65, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.01, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=0.21, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=40, Total reward=-0.64, Steps=40960, Training iteration=19\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.27\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/21_Step-40961.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=41, Total reward=-3.05, Steps=41691, Training iteration=19\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01929544098675251, KL divergence=[0.], Entropy=[-0.24299921], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03665941581130028, KL divergence=[0.], Entropy=[-0.2429911], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03277822956442833, KL divergence=[0.], Entropy=[-0.24301009], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.040378496050834656, KL divergence=[0.], Entropy=[-0.24301209], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02998800203204155, KL divergence=[0.], Entropy=[-0.24301119], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.024873310700058937, KL divergence=[0.], Entropy=[-0.24302272], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.019742676988244057, KL divergence=[0.], Entropy=[-0.24302484], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03340623527765274, KL divergence=[0.], Entropy=[-0.24302182], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02756982296705246, KL divergence=[0.], Entropy=[-0.24303694], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03766647353768349, KL divergence=[0.], Entropy=[-0.24304275], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=42, Total reward=-2.53, Steps=42422, Training iteration=20\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=42, Total reward=-0.52, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=42, Total reward=-0.5, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=42, Total reward=-0.12, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=42, Total reward=-0.1, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=42, Total reward=-0.23, Steps=43008, Training iteration=20\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.29\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/22_Step-43009.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=43, Total reward=-2.98, Steps=43739, Training iteration=20\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028742752969264984, KL divergence=[0.], Entropy=[-0.24304293], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030330955982208252, KL divergence=[0.], Entropy=[-0.24305859], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03801841661334038, KL divergence=[0.], Entropy=[-0.24306689], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03761772811412811, KL divergence=[0.], Entropy=[-0.24306847], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.025461940094828606, KL divergence=[0.], Entropy=[-0.24308325], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01765601523220539, KL divergence=[0.], Entropy=[-0.24312271], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.036477651447057724, KL divergence=[0.], Entropy=[-0.2431569], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.022116631269454956, KL divergence=[0.], Entropy=[-0.24317098], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03366559371352196, KL divergence=[0.], Entropy=[-0.2431995], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04091690108180046, KL divergence=[0.], Entropy=[-0.24321376], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=44, Total reward=-2.45, Steps=44470, Training iteration=21\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=-0.47, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=-0.08, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=-0.11, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=-0.21, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=44, Total reward=-0.45, Steps=45056, Training iteration=21\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.26\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/23_Step-45057.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=45, Total reward=-2.79, Steps=45787, Training iteration=21\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0306970477104187, KL divergence=[0.], Entropy=[-0.2432267], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03696691244840622, KL divergence=[0.], Entropy=[-0.2432341], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03790518641471863, KL divergence=[0.], Entropy=[-0.24322815], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.058992255479097366, KL divergence=[0.], Entropy=[-0.24324593], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04299887269735336, KL divergence=[0.], Entropy=[-0.24328232], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.020764455199241638, KL divergence=[0.], Entropy=[-0.24329457], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02925027161836624, KL divergence=[0.], Entropy=[-0.2433063], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04052002727985382, KL divergence=[0.], Entropy=[-0.243315], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03401249647140503, KL divergence=[0.], Entropy=[-0.24332161], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.040614042431116104, KL divergence=[0.], Entropy=[-0.24333316], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=46, Total reward=-2.29, Steps=46518, Training iteration=22\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=46, Total reward=-0.16, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=46, Total reward=0.11, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=46, Total reward=-0.23, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=46, Total reward=0.33, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=46, Total reward=-0.02, Steps=47104, Training iteration=22\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = 0.01\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/24_Step-47105.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=47, Total reward=-2.69, Steps=47835, Training iteration=22\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.036067262291908264, KL divergence=[0.], Entropy=[-0.24335462], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.021119501441717148, KL divergence=[0.], Entropy=[-0.24339093], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.011448245495557785, KL divergence=[0.], Entropy=[-0.24342093], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.023843469098210335, KL divergence=[0.], Entropy=[-0.2434522], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.029043175280094147, KL divergence=[0.], Entropy=[-0.24346972], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02234245464205742, KL divergence=[0.], Entropy=[-0.24349175], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.025212615728378296, KL divergence=[0.], Entropy=[-0.24352191], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02625477872788906, KL divergence=[0.], Entropy=[-0.24355532], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03855782374739647, KL divergence=[0.], Entropy=[-0.24357814], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03131304681301117, KL divergence=[0.], Entropy=[-0.24360552], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=48, Total reward=-2.61, Steps=48566, Training iteration=23\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=-0.19, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=-0.43, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=-0.7, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=-0.19, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=48, Total reward=-0.35, Steps=49152, Training iteration=23\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.37\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/25_Step-49153.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=49, Total reward=-2.53, Steps=49883, Training iteration=23\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03671504929661751, KL divergence=[0.], Entropy=[-0.24361697], training epoch=0, learning_rate=0.0001\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPolicy training> Surrogate loss=0.04346982389688492, KL divergence=[0.], Entropy=[-0.24362893], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04147516191005707, KL divergence=[0.], Entropy=[-0.24366535], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01816238835453987, KL divergence=[0.], Entropy=[-0.24369612], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.022480951622128487, KL divergence=[0.], Entropy=[-0.24372528], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03072584792971611, KL divergence=[0.], Entropy=[-0.24375315], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017373325303196907, KL divergence=[0.], Entropy=[-0.24378549], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.019901566207408905, KL divergence=[0.], Entropy=[-0.24382019], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02278904803097248, KL divergence=[0.], Entropy=[-0.2438579], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02648627944290638, KL divergence=[0.], Entropy=[-0.2438871], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=50, Total reward=-2.57, Steps=50614, Training iteration=24\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=50, Total reward=-0.28, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=50, Total reward=0.26, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=50, Total reward=-0.46, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=50, Total reward=-0.24, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=50, Total reward=-0.77, Steps=51200, Training iteration=24\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.3\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/26_Step-51201.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=51, Total reward=-2.38, Steps=51931, Training iteration=24\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017769450321793556, KL divergence=[0.], Entropy=[-0.24391168], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03175228461623192, KL divergence=[0.], Entropy=[-0.24392834], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.025863928720355034, KL divergence=[0.], Entropy=[-0.2439395], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.023628337308764458, KL divergence=[0.], Entropy=[-0.24394403], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.04122862592339516, KL divergence=[0.], Entropy=[-0.24394582], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02875806763768196, KL divergence=[0.], Entropy=[-0.24394926], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03410770744085312, KL divergence=[0.], Entropy=[-0.24397157], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030271464958786964, KL divergence=[0.], Entropy=[-0.2439604], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.022111298516392708, KL divergence=[0.], Entropy=[-0.24396673], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.022476667538285255, KL divergence=[0.], Entropy=[-0.24398327], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=52, Total reward=-2.42, Steps=52662, Training iteration=25\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=-0.38, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=-0.27, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=-0.52, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=-0.15, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=52, Total reward=-0.24, Steps=53248, Training iteration=25\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.31\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/27_Step-53249.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=53, Total reward=-3.04, Steps=53979, Training iteration=25\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.026407087221741676, KL divergence=[0.], Entropy=[-0.24399868], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02406737394630909, KL divergence=[0.], Entropy=[-0.24402909], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02782568521797657, KL divergence=[0.], Entropy=[-0.2440621], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.027128690853714943, KL divergence=[0.], Entropy=[-0.24408938], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03395025059580803, KL divergence=[0.], Entropy=[-0.24410605], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02552124112844467, KL divergence=[0.], Entropy=[-0.24411581], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.024106813594698906, KL divergence=[0.], Entropy=[-0.2441168], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028101416304707527, KL divergence=[0.], Entropy=[-0.24412933], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.017681611701846123, KL divergence=[0.], Entropy=[-0.24413978], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.032554689794778824, KL divergence=[0.], Entropy=[-0.24415006], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=54, Total reward=-2.72, Steps=54710, Training iteration=26\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=54, Total reward=-0.17, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=54, Total reward=0.06, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=54, Total reward=-0.13, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=54, Total reward=-0.3, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=54, Total reward=-0.17, Steps=55296, Training iteration=26\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.14\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/28_Step-55297.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=55, Total reward=-2.65, Steps=56027, Training iteration=26\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.047147203236818314, KL divergence=[0.], Entropy=[-0.24415185], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.022395340725779533, KL divergence=[0.], Entropy=[-0.24418421], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.032668959349393845, KL divergence=[0.], Entropy=[-0.2442142], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.023071520030498505, KL divergence=[0.], Entropy=[-0.24422598], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03296980634331703, KL divergence=[0.], Entropy=[-0.24423502], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028853099793195724, KL divergence=[0.], Entropy=[-0.24425942], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02790745534002781, KL divergence=[0.], Entropy=[-0.2442953], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02471843548119068, KL divergence=[0.], Entropy=[-0.24431047], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03196879103779793, KL divergence=[0.], Entropy=[-0.24431954], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030333368107676506, KL divergence=[0.], Entropy=[-0.24431348], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=56, Total reward=-2.52, Steps=56758, Training iteration=27\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=0.04, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=-0.06, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=-0.59, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=-0.44, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=56, Total reward=-0.74, Steps=57344, Training iteration=27\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.36\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/29_Step-57345.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=57, Total reward=-2.89, Steps=58075, Training iteration=27\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.034931521862745285, KL divergence=[0.], Entropy=[-0.24430871], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0314793735742569, KL divergence=[0.], Entropy=[-0.24433346], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.019002867862582207, KL divergence=[0.], Entropy=[-0.24435773], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.030090494081377983, KL divergence=[0.], Entropy=[-0.24437751], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01089328434318304, KL divergence=[0.], Entropy=[-0.24438876], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.016949832439422607, KL divergence=[0.], Entropy=[-0.24441715], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.0211087204515934, KL divergence=[0.], Entropy=[-0.24443994], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02933698706328869, KL divergence=[0.], Entropy=[-0.24442998], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.016978751868009567, KL divergence=[0.], Entropy=[-0.24443322], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.016536157578229904, KL divergence=[0.], Entropy=[-0.24443951], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=58, Total reward=-2.53, Steps=58806, Training iteration=28\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=58, Total reward=-0.12, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=58, Total reward=-0.43, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=58, Total reward=-0.11, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=58, Total reward=-0.1, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=58, Total reward=-0.01, Steps=59392, Training iteration=28\u001b[0m\n",
      "\u001b[31m## agent: Finished evaluation phase. Success rate = 0.0, Avg Total Reward = -0.15\u001b[0m\n",
      "\u001b[31mCheckpoint> Saving in path=['/opt/ml/output/data/checkpoint/30_Step-59393.ckpt.main_level.agent.main.online']\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=59, Total reward=-3.07, Steps=60123, Training iteration=28\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.026723189279437065, KL divergence=[0.], Entropy=[-0.24444532], training epoch=0, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.02606072463095188, KL divergence=[0.], Entropy=[-0.24446616], training epoch=1, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03448633849620819, KL divergence=[0.], Entropy=[-0.2444923], training epoch=2, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03602318838238716, KL divergence=[0.], Entropy=[-0.24453397], training epoch=3, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.028769008815288544, KL divergence=[0.], Entropy=[-0.2445754], training epoch=4, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.036359820514917374, KL divergence=[0.], Entropy=[-0.24459983], training epoch=5, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.03063923679292202, KL divergence=[0.], Entropy=[-0.24461724], training epoch=6, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.025110460817813873, KL divergence=[0.], Entropy=[-0.2446288], training epoch=7, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01895006373524666, KL divergence=[0.], Entropy=[-0.2446537], training epoch=8, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mPolicy training> Surrogate loss=0.01864372007548809, KL divergence=[0.], Entropy=[-0.2446797], training epoch=9, learning_rate=0.0001\u001b[0m\n",
      "\u001b[31mTraining> Name=main_level/agent, Worker=0, Episode=60, Total reward=-3.06, Steps=60854, Training iteration=29\u001b[0m\n",
      "\u001b[31m## agent: Starting evaluation phase\u001b[0m\n",
      "\u001b[31mTesting> Name=main_level/agent, Worker=0, Episode=60, Total reward=-0.57, Steps=61440, Training iteration=29\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.m4.4xlarge\"\n",
    "estimator = RLEstimator(source_dir='src',\n",
    "                      entry_point=\"train-coach.py\",\n",
    "                      dependencies=[\"common/sagemaker_rl\"],\n",
    "                      toolkit=RLToolkit.COACH,\n",
    "                      toolkit_version='0.11.0',\n",
    "                      framework=RLFramework.MXNET,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      output_path=s3_output_path,\n",
    "                      base_job_name=job_name_prefix,\n",
    "                      hyperparameters = {\n",
    "                          \"RLCOACH_PRESET\" : \"preset-portfolio-management-clippedppo\",\n",
    "                          \"rl.agent_params.algorithm.discount\": 0.9,\n",
    "                          \"rl.evaluation_steps:EnvironmentEpisodes\": 5\n",
    "                      }\n",
    "                    )\n",
    "# takes ~15min\n",
    "# The log may show KL divergence=[0.]. This is expected because the divergences were not necessarily required for \n",
    "# Clipped PPO. By default they are not calculated for computational efficiency.\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job name: rl-portfolio-management-2019-06-12-21-36-03-778\n",
      "S3 job path: s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778\n",
      "Output.tar.gz location: s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/output/output.tar.gz\n",
      "Intermediate folder path: s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/output/intermediate/\n",
      "Create local folder /tmp/rl-portfolio-management-2019-06-12-21-36-03-778\n"
     ]
    }
   ],
   "source": [
    "job_name=estimator._current_job_name\n",
    "print(\"Job name: {}\".format(job_name))\n",
    "\n",
    "s3_url = \"s3://{}/{}\".format(s3_bucket,job_name)\n",
    "output_tar_key = \"{}/output/output.tar.gz\".format(job_name)\n",
    "\n",
    "intermediate_folder_key = \"{}/output/intermediate/\".format(job_name)\n",
    "output_url = \"s3://{}/{}\".format(s3_bucket, output_tar_key)\n",
    "intermediate_url = \"s3://{}/{}\".format(s3_bucket, intermediate_folder_key)\n",
    "\n",
    "print(\"S3 job path: {}\".format(s3_url))\n",
    "print(\"Output.tar.gz location: {}\".format(output_url))\n",
    "print(\"Intermediate folder path: {}\".format(intermediate_url))\n",
    "    \n",
    "tmp_dir = \"/tmp/{}\".format(job_name)\n",
    "os.system(\"mkdir {}\".format(tmp_dir))\n",
    "print(\"Create local folder {}\".format(tmp_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/output/output.tar.gz...\n",
      "Downloading rl-portfolio-management-2019-06-12-21-36-03-778/output/output.tar.gz\n",
      "Copied output files to /tmp/rl-portfolio-management-2019-06-12-21-36-03-778\n",
      "Checkpoint directory /tmp/rl-portfolio-management-2019-06-12-21-36-03-778/checkpoint\n",
      "info directory /tmp/rl-portfolio-management-2019-06-12-21-36-03-778/\n",
      "CPU times: user 141 ms, sys: 28.7 ms, total: 170 ms\n",
      "Wall time: 1.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wait_for_s3_object(s3_bucket, output_tar_key, tmp_dir)  \n",
    "\n",
    "if not os.path.isfile(\"{}/output.tar.gz\".format(tmp_dir)):\n",
    "    raise FileNotFoundError(\"File output.tar.gz not found\")\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "os.system(\"aws s3 cp --recursive {} {}\".format(intermediate_url, tmp_dir))\n",
    "if not os.path.isfile(\"{}/output.tar.gz\".format(tmp_dir)):\n",
    "    raise FileNotFoundError(\"File output.tar.gz not found\")\n",
    "os.system(\"tar -xvzf {}/output.tar.gz -C {}\".format(tmp_dir, tmp_dir))\n",
    "print(\"Copied output files to {}\".format(tmp_dir))\n",
    "\n",
    "checkpoint_dir = \"{}/checkpoint\".format(tmp_dir)\n",
    "info_dir = \"{}/\".format(tmp_dir)\n",
    "\n",
    "print(\"Checkpoint directory {}\".format(checkpoint_dir))\n",
    "print(\"info directory {}\".format(info_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for s3://sagemaker-us-east-1-412868550678/rl-portfolio-management-2019-06-12-21-36-03-778/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv...\n",
      "Downloading rl-portfolio-management-2019-06-12-21-36-03-778/output/intermediate/worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "csv_file_name = \"worker_0.simple_rl_graph.main_level.main_level.agent_0.csv\"\n",
    "key = os.path.join(intermediate_folder_key, csv_file_name)\n",
    "wait_for_s3_object(s3_bucket, key, tmp_dir)\n",
    "\n",
    "csv_file = \"{}/{}\".format(tmp_dir, csv_file_name)\n",
    "df = pd.read_csv(csv_file)\n",
    "df = df.dropna(subset=['Evaluation Reward'])\n",
    "# print(list(df))\n",
    "x_axis = 'Episode #'\n",
    "y_axis = 'Evaluation Reward'\n",
    "\n",
    "plt = df.plot(x=x_axis,y=y_axis, figsize=(12,5), legend=True, style='b-')\n",
    "plt.set_ylabel(y_axis);\n",
    "plt.set_xlabel(x_axis);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:15: FutureWarning: 'argmax' is deprecated. Use 'idxmax' instead. The behavior of 'argmax' will be corrected to return the positional maximum in the future. Use 'series.values.argmax' to get the position of the maximum now.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd700133518>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEsCAYAAAAitRNEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzsnXd4HNX1sN+76r1YsmzJvTcwLtgU01sg9E5CS6hJICGEEAKBOBBSPkiBBEJI4EeH0IsxYJrBGIwbxr032bJk9d73fn+cmd3Rane1slfSSrrv8+wzfebO7uyZc8859xyltcZgMBgMfRtXTzfAYDAYDF2PEfYGg8HQDzDC3mAwGPoBRtgbDAZDP8AIe4PBYOgHGGFvMBgM/QAj7HsApdTxSqk9/eW6/RGl1FNKqd/3dDu6GqXUMUqpTT3dDkPHGGFviFiUUicrpVYqpWqVUnuUUhcH2G+wUuptpVSBUkorpUb4bI9TSj2plKpSShUqpW51bBuqlFqilCpTSv3F57j3lFIzu+LeeivW9zvGXtZaL9Jaj++G656klNqolKpTSn2qlBoeZN9PlVLF1u/9rVLqHMe2O5VSNY5PvVLKrZTKsrY/qJTaopSqtq53ZYBrXGl9F9eG/267BiPsIwylVHRPtyESUEpNAl4A7gLSgKnAigC7u4H3gQsCbJ8LjAWGAycAtyulvmNt+zXwNDASONcW7kqpS4AdWuvlB30zB0l3PROR+uxZgvh14G4gE1gO/C/IIT8DBmutU4HrgeeUUoMBtNZ/0Fon2x/gz8BCrXWJdWwtcBbyzF0FPKSUOsqnPRnAncC6cN1jt6C17vMfYCfwS2A18mM+AeQA7wHVwEdAhmP/V4BCoBL4HJhsrY8FVgE3W8tRwGLgng6unwA8BZQD66227PFp36+s9jUC0cAdwDarfeuB8xz77wJmWPPfB7SjjdcAb4Z43YnAQqACeXDPttaPtNa5rOX/APsdxz0L3GLNLwTus76HamABkBWG3+wF4L5OHhNtfRcjfNYXAKc6lu8DXrLm3wPGW/MvARcDqcA3QHoH11PA34D9QBWwBphibXsKeAR41/pevgZGO459CMi3jlsBHOPYNhd4FXjO2n6tY93/rPOtBKY6jskFXgOKgR3AT0P4vvxdZxbwlfX77wP+CcRa+39ufb+1QA1wCXB8KM/UQT4L1wNfOpaTgHpgQgjHzgIagFkBfr/twFVBjn8b+IXPuseAH1v3ee3B3l93fXq8Ad1ykyJMlyACPs/6c64EpgHxwCfAbx37/xBIAeKAvwOrHNumIMJzIqJ1LgGiOrj+n4BFiFYyFFhLe2G/ytqWYK27yPoDu6w/VS2irQA8Yz+AwOPIS+FHjm0/7+i6QAywFdFQYoETESFiC77deF8om6w/xUTHtmnW/ELr+uOQl8tC4E+Oe6sI8rkjyHe2HRHKaxCh8xyQ2cH33E7YAxnWuhzHuguBNdb8A8BNQDqwBZiMCOKrQniuTkMEdToiOCY6fqOngFJE2EQDz2O9YKztlwMDrG2/QJSLeGvbXKAZONf6/RMc6y60frvbEKEeY+2zArjH+i1HWd/faR203991ZgBHWO0aAWzAerFbx2hgjGP5+E48U3cEex6CtPMh4F8+69YCFwQ5Zh4i5DXS63P52edY5KWVHOAcCdaz9x3HullIz8KFEfaR90GE6fcdy685Hx7gZixt2M+x6dYDk+ZY9wtEAJYDY0O4/nafB+Z62gv7H3ZwjlXAOdb8NcDb1vwGRCOzNdVdwPSOrgscgwgYl2P7i8Bca/5Z4FZgkHWv/w+4kfZa/0LgN45z/Bh4Pwy/WZP1vYwDkq3f7PkOjvEn7Ida6+Id604BdlrzmYi2/C3wc0QB+NRa/wKizd4U4HonApsR4ejy2fYU8F/H8hnAxiBtL8fS1BEh/LnP9rnAEseyCxFExwCzgd0++/8a+L8Ovq921/Gzzy3AG47lYMI+6DN1EM/CEzgUCGvdYuDqDo6LAU4Hbg1y3qeCHP808qJQ1nIUIuiPcDz7vUbY9yebfZFjvt7PcjKAUipKKfUnpdQ2pVQVInAAshz7P43Yf+drrbeEcO1cpMtus8vPPs7ttgNolVKqQilVgfQo7DZ8Bhxj2SGjgJeBoy3HZBryYujourlAvtba7bM9z3GN4xHt53PkwT7O+izyOa7QMV+H9V2GilLqMYfD7E5rdT0irDZrrWuAPyACs7PUWNNUx7pURONEa12mtb5Eaz0V0SD/gbz870C0x5OBG5VSE31PrLX+BDFzPALsV0o9rpRyXifg96KUuk0ptUEpVWn9vmm0fcbaPA++66zvfw/yOw4Hcu1nxTrfnUhPtiN8n7txSql5liO7Cvnes/wf2o6OnqkOUUoNczpQrdU1tP39wPEbBkJr3ay1fg84VSl1ts91EpHe89MB2vEA8p+7WFuSHVFkVmutl4R6P5FEfxL2ofI94BzkT56GdGVBuuk2jyLdxNOUUnNCOOc+RMO0GeZnH/uBwoo0+A9iXhigtU5HBI8C0FpvRYTHzYhmVoUIluuBLxx/tmDXLQCGKqVcPtv3WvOfIZra8db8F8DRiLD/LIR7tu+lJsjnTut+btRep9kfrENXO78Tn/mQ0VqXI9/DVMfqqfh3rl2PaM9rgUOA5VrrJsSUdEiA8z+stZ4BTEJ6Ib/sqE1KqWOA2xH/QIb1+1bS9hnzd7+e39L63YYgv2M+4kxOd3xStNahvBx9r/MvYCPSY01FXhqq3VH+CfpM+YmEqfEV7Frr3bqtAxXkt/L8fkqpJGA0oTtIo639nZwHlCFKTBuUUr9DegSnWv8tm5OA86wXYSFwFPAXpdQ/Q2xHj2KEfXtSECdpKZCIaDYelFJXIHbNq4GfAk8rpTrSZF8Gfq2UylBKDUGEdDCSkD9hsXXNHyBahpPPkJeBLXgX+ix3dN2vkRfG7UqpGKXU8UgUwksAVo+lHrEtf2Y99EVIxEvIwt75x/Xz+UOQQ/8P+IFSapSlhd2BvGD9opSKR3wsAHHWss0zwG+s72ECcB1iZnEePxD4CWLaALGHn2D9tjMRk5jvNQ9XSs1WSsUgPpUGJDKoI1KAFuT3jVZK3UN7zdUfM5RS51tRM7cgz+kSYClQrZT6lVIqweqdTlFKHR7COf21rQqosb6rH/lsL0J8Av7o6Jn6Q7DnIUib3gCmKKUusH7XexANe6PvjkqpCUqp063vIUYpdTnSO/V9Zq8CnnFo7fbxv0YUvpO11qU+x1yN+GUOsz7Lgd8hvruIxwj79jyDdD33IhEsni6bUmoY4rC9Umtdo7V+AfnB/9bBOX9nnXMHEq3ybLCdtdbrgb8gURFFiFa52Ge3z5A/5ucBloNe19JYz0I0mBKkt3Klzx/oM6BUa53vWFaIc7tL0Vo/ifwWX1v30Ii8XAFPj+EYxyH1eE02G61lm98iTuRdyD08oLV+3+eSDwL3WiYjgD8iNvl84B3tPwQzFemBlVvnLkUcvh3xAWIL3mwd14B/s40vbyHO+nLgCuB8y1TRCpyJCKAdyO/5X6Rn2lluQ4RdNXJvviGOcxEFp0L5jHsI8ZnqNFrrYkTJuB+599nApfZ2ywz4mL1otXE/8jL9GXCJ1nqlY/885Ld9xs/l/oD0Rrb66YFWaK0L7Q/iV6rSWlcezP11F8rnxWYwGCIQpdRcxDF6eU+3xdA7MZq9wWAw9AOMsA8TSobWB3RCGgw9gXkuDTbGjGMwGAz9AKPZGwwGQz/ACHuDwWDoB/RYlrusrCw9YsSInrq8wWAw9EpWrFhRorXO7uxxPSbsR4wYwfLlPZ491mAwGHoVSil/6VY6xJhxDAaDoR9ghL3BYDD0A4ywNxgMhn6AEfYGg8HQDzDC3mAwGPoBRtgbDAZDP8AIe4PBYOgHGGFvMBgim3k/hy86Khlh6IgeG1RlMBgMHdJQBcuflPk5P+/ZtvRyjGZvMBgil/yve7oFfQYj7A0GQ+RS8E14zrPtE2iu73i/PowR9gaDITLRGta/5V0+UGH955Hw7Hnw4T3haVcvxQh7g8EQebhbYfFDULQWRlh15Wv2Q1NdJ8/jhvoymS/ZHN429jKMsDcYDJHHB3fBR7+F0SfCET+Sde/fAX8aCvnLQj9PU413PiYxvG3sZRhhbzAYIgu3G/augKGz4fLXIWWwrN80H9wt8NU/Qz9XQ6V3ftN8aG4Ib1t7EUbYGwyGyKFgFfx1IuxZCskDQSmZOtn8voRkhoJT2APs+Cw87eyFGGFvMBgig80fwBOnQk2hLEfHyzQxy7vPiGOgpQGW/huqCzs+Z6P1UjjqpzIt3Ra+9vYyjLA3GAyRwTfPQmImnPRbWW5tkmlMPMSlyfzEsyFtGHzye/jLeP/nKVwD7/wMNr3v1ewnnwvxaVC6pWvvIYIxwt5gMEQGtSUwYAxkjZXllibvtmSr5GpqLow92bu+Zr93fs2rMtr2PyfCiqdg5TNec098OmSNg5Itsv3dX3TprUQiRtgbDL2d1hZY9aJMeyt7lsPuryApG6LiZF2rQ9gnWXb71MEw84fe9Zvek+nKZ+C1aySPTmsTZE+ATe96Nfn4NBgwFnYuEufvsv92/T1FGEbYGwy9nZ2fw5s3wro3erolB4bbDf89yZpvgYETZH7SOd59bM0+JRcGHQK/rRABXrga6spEyCtLnOXNhOQcmf/8AZnGpULWmK6/lwjGCHuDobdTuUemWz7o2XYcKPvXe+eVC9KHwV2FMP1K7/rkHFBR3sgcpSB5kJhxdn4hL4lBh8q28afLsk10AkTHihnHJiW36+4nQulQ2CulnlRK7VdKrQ2wXSmlHlZKbVVKrVZKTQ9/M32oKxMHTNn2Lr+UwdDj7Pu27cjRpf/xmi8AqgpkuuXDzplyXr9BNF93a3jaeaDsXSHTE+6CMyxNPCZBBLrN4dfCuY+CK8q7Lnkg1BZD8UZZPucROORimH0jnPWwd7/4VJkOGOtd19L/8uSEotk/BXwnyPbTgbHW53rgXwffrA5Y9l9xwMy7tcsvZTD0KFs+hH8fC5/9WZa1hvm3wYuXevep2ivThgq4bwAU+tXL2tLSBKtfkqiWL/8R/nZ3hqK1EJsMx9zWPqbeJns8TL207brkgVBTBPXlcvygKXDBfyAuWUw2oy3TULwVyZM5SvaLTYbGavku+xEdCnut9edAWZBdzgGe0cISIF0pNThcDfTLhrdlGkqcrcHQW9m7Ap6/UOYLVsrU2Zu1zTfFm2HgJO/6Vc93fO7ynd75nV8cVDMPmsK1kDMZXJ20KicNhJpiEfYJme2323Z7W9hHx8Kde+HY28TM09K/RtOGw2afB+Q7lvdY67qGqgKJowXcFbv63dvZ0E9wt4rT0aZsh0z3rfKu+9tk2DhfzDwjj4UfLxEBWLCKDnEmBYtNCk+bDwStRbPPmdL5Y5OzoalaejYJ6e232+abuNS26+NSZNpY3flr9mK61UGrlLpeKbVcKbW8uLj4wE6y9SMAXm45DldznbzVDYa+xvInRYjbVObLACHnOoC3fiL259zpMHAiHHox7F3ecQ6YwtXiDB0yy2sG6gkqdsko10GHdP5YW3Mv3gwJGe2320I93lfYW5p+P5Md4RD2e4GhjuUh1rp2aK0f11rP1FrPzM7OPqCL1cXn8Lr7WD52T5MVFbsO6DwGQ8SiNXz2/7ypfW32b2ivtdvpe/OsuIgRcyTOfO/y4Nco+EZi0bPGes1BPYHtXzgQYW/H3tcUyshbX2KTZRoV23a97RdwDsjqB4RD2L8NXGlF5RwBVGqt94XhvH6ZVzuJW5tuZI+2frDyXbB7SVddzmDofmqKoHY/TDwLrvsUrrQKeCz5lyTyGnsqHP9ruN6R1CtztEyHHQEo2Lk48Pm1FmGfOw3Shojvq7W5y24nKIVrACW9ks6S7FAY04e3325r9spHzKUMkmlNUeeveTB0Nhd/mOmw4LhS6kXgeCBLKbUH+C0QA6C1fgyYD5wBbAXqgB90VWMB8jISuHjmEEqKY6AIqT5TsQu+9wqMO7UrL20wdD01xd6cLwMnisZu+6XWvynT6VfKiwDg+oWSAth2biZkiLM2UO3WplpY+7qELOZOg+g4QIsvLMOPwOxqitbCgNEH5jdIckTuDDm8/XZbo/cV9h7NvhuFfdF6+NeRcMlz3t+um+lQ2GutL+tguwZ+ErYWdcDRY7I4ekwWNzyzjFoSSbLNOD1pdzQYwoUdc56YJfZ0kHjzkceJVn/dJ5A3w7t/7rT25xgyU14Mbnf7CJcP7pSwZftYOytk5Z6eEfaFa7wmqM6S5NDss/0kRdNumfoK+/h0ScnQndF8+Zb1Yf3bPSbse+0I2vTEOAqUI/Wpr13OYIhktG5fU7W+XHK3ANy0TLI92lz4f3DrxraCPhBDZ4kz11+Gx+0O00/OZEgdIvM9YbdvqJRe+YFE4oCEUtqk+In21tZgMV9hr5Q4d7vTZm9/v86BYt1MLxb2MexpHeBd0VwHnz0A794mI2wNhkhm6X/gfmu4f/lOWP2KFMX+6p8iuHwdjkkDJAlYKNgmjT0+TtrWFhE6w+fAmX+XUappVpR0VQ8I+6J1Mj0Q56wvccnt1w2dLVN/mnTyQG/e/O6gynJj7t8AC37TudKKYaJDM06kkpoQQ4E70/u6mn+bd2PZdrji9R5pl8EQEhvfkenuJfLnt82RE8+GU+87uHNnjJCpbdpsboD1b4m5xN0Mh30Ppn1ftsUmyYCkntDsC76RqZ3TJtzkTIZ7yv0P1koZ1L3pVpqsmP6itRL2OmAsDPXjZ+hCerVm/7eWC/1vLN/RvY0xGDpL2jCZFq1rGz48dJZXWB8o0XESS15bIssLfgNvXA9LH5dlO6ukpy15XSPsWxph5bPiFPbHzsWQMTL0Hos/rp4P33818PZAo3KTB7a12bvdEq/fVdjfge1HsCOFupHeK+wTYikhjc3XOOySJ/5GppmjeqZRBkOo2I5RX7u6PbT/YEnKkogb8PoBNr0PrmgYOLntvmlDu0bYr3kF3r4Jnvqu98XjZN8qebkdDCOOhrGndP645BwZo2AXSFn8N3jkcImaCTetLe1fJL6jeruBXivs0xJiAKhosSxRKgqO/SWMOdnY7A2Rx9rXJVOrHUZZVypTO3vlcXfI1F8I4YGQlA11loBtrJFp5W4Jy3Q6fkFi7Uu2wGvXSeK1cLFnmaQXLloP83/ZdltDlZiZsif4P7arsUff2i/E7Qtl2hWmnY9+Kz4RZ/SQ76jebqDXCvv0REvY1zXBz1bD7VYh4YRM7x/JYIgUXrtGQh5toWILmeY6UVSO/qkU5DiQwUX+SMqCinzR2J2OyNzD2u+bNkRs+WtehhcuDs/1QUxUQ2bCUTfDutfh+Yu86ZTLrP9r1tjAx3cltrD3ddJW7O7cebSGje8GH5S2ab5MEwdIzwqMGaczeDT7+maJD7ZzYyQOMJq9IbLQGmKtP/eiv8jUadYYdIg4SsMZlpc9QXxXf5vctpDHYD/CPtvxgtHu8OW3L98JmSPhiB/L8pYF3mybdm1YfzltuoMUS9hvnC9tshPNFW/o3Hk2zYeXvhc8TbSycvA313n9MUbYh06apdlX1fu8URMHiOfbWazYYOhJvnkWGitFAO9cJPbbeodCMugA48yDYfcQxp8B5zwKk8+XZX+DsHyjQsKRb6qxRnovGSMkbHTaFbK+oVKmtsOypzJu2uMLFj0ID02VRHMA+Us7dx47e6g/n4SNrc03VElBdTA2+86QEhdNlEtRUecr7C1Nod5o94YeoroI/ne5V1tc+7rkrjn7n7Jsj6acfJ5MR50Q/jZMPg/O/w9c/IyEWQ4/ShShnMnt903IgEtflIFbIPZ7Xxpr4N1fhN5rtl8YtiZ72Pes81gafbOVJyamh4R9So63R/Pdv0jaiRN/I1WvOpMN097XX5y/L0f8WEb6umK8Sdq6kV4bZ6+UIi0hhop6Hw0+0RpoVVfqTXhkMITCZw/AyGOsZGIHiNbw+nWS2mDDO6LJNVTCuNO8UWJ23ppJ58Cp90NqF9RDdUVJumObmdfAYd+3cuH4YcIZXkFeslna62T9m1IhTms4868dX3+NFQ7pMVtYmmw7zT6x43N1Fdd+JCYuOxe+7cjOXxZ6nq0ay/cSLDd+XSlMvwqO/5X0AMac3PlCLWGg12r2IHb7ynqfmpseYW80e0OI1BRLRslPfw+vXnNw59rwjgh6m9KtYs5IGiijYmOT4ZvnZFtStsS4d8cQeperY8GamCk5eUr8xJvbdv/iTR1fy+2GL6wXQsZImdohpbaw92j2PSjs45LbFj3Jmy729UBJ5GzKd8E6KymdrdkHkjfuVomKshXPpCwpNNMD9HphX1HXREGFI8eIXZ7MROQYQuWZs+F9K/QxJuHgzrXsv6LN/mw1nHKvd31Stgh1Z/RJYla7w3ucrHGSY37xw20LoNjD/UMJTXSaUG0HrEfYW2acJkuL7skqWb7EJomzvCNh//jx8MpV8lKz79WOrvKltlic3oFq63YjvVrYpyfGsGhLCUf96ROW7rC+dKcZx2DoiNZm2G8NpMmeKGlvtZYuvdvduXPlLxWtftI5EiF29M9g3Omyzf6zX/S0d/+kAyvg06VkjZV6tx/eLY5lGzv1QnVBcGckeBOMnfWQt9cSmwwohxmnThyXkZbAcNgRklMoWCilLeAbK72a/f4AUTx2GmU71LMH6dXC3g6/BNhYaGkMdgIp46A1hIJd+OaS52HmD8WBWLYd/pgHn/+/tvu2NsOX//Rqp768YpVyGOHopk88U6a2sHemEe6psMNgOEe02tki3a2w9WPv+o4UqVpL2NuRJyBmpPjUtmacmDCHm4aDobOkzOPelR3vW1/uFfbVBWLe8cV+8Rlhf3CkO4S9sh+a6DjRIozN3hAKWxZIdMSo4yDLEk7/u1ymK55uu++6N2HBXfDpH/yfKyZenr0xJ3nXHXIxnPUwDDuq/f494KTrkEMulqyY4DW17PhchNlh1vcS6GVnYwu4JB/TRXyaNxqnqbZnnbOBGHOy5Lp/8tTg1b5ABsjVl8MhF4mt3/bFODGafXgYP8gbq9rY3EpFXRNVDc2i3RszjiEUtn0Cw4+UQS5Z42Sdbdbx1bztkEmnTfej38GjR4npp65UImCc2mp0LMy4CqIcgW/xDqdgpBEdC1fPE/NKXamEjz57rmybeolMbe08EB5t1sdMFZfmo9lHoLCPTxMnKsAz57Tf7oy6mfdzcVwPOlRMcv5SJnuEvbHZHxSnT/GGVv7+3Q0cdu+HnP73RWYUrSE0WhrF1mrno0lxhECmDpGh83YuG5D0AyA27epC2fbFX2H/Oqm4VF/e9hyB+Nm3cJufWPZIQSlxHteWwu6vZN2kc7zaaWMHwr52v7wsfF9q8WltQy8jUbMHb2ZKtx+7vf0MOEnMlHvxrTHbXC/2/7i0g3f8h4FeLewzkmIZn9N22PHeinp21ccbzT7SWPmMlGQLxLo3oWBV4O0F38CyJ8Lbpv0bpJqRPbjGaVaZc4uMxK4q8K6rLoB0KzXxxnltn7H3fy3TUHLbJKRHhKYXlAGjYe2r8OaPZPmsh9vHygeiptgbfeQkPs0RjVPbIwOLQiJYvp5KP8I+IUP8D80+wv7lqySdQlJkRF31amEP8O5P53DXGRO5Zs5Iz7pvSlxG2AeiplhMF91Zkk1rePtmePkK7zq326s1u1sllO3x4/wf726VcLd3b5XRqeFiyb/EXu90Sp54N8y63lvTtMQRV15dKKNdB4yRF5fTIbfrCxh+NEz4bvja15Oc+6+2Q/rj09rHygeipsj/y6ydgzZCNfsLnpTwWeVqH5FlJ0o7xDFgLSHD0uxr2u675QOZRkjEUa8X9tFRLq47dhR3nzmJuWdNkpWJAzo35Lm/sOIpeHCMlL/7/MHuu65vrvTnL4J7M0W4v3CJzAfDGRlhZ40MBzu/kJJ1zgiZY2+DMx6ALEvYv3advGya6iRmOjVX8szs+AzWvuY9zhUjpf4iLbrkQEkfCj9x+CaUElOEK7pjB23t/vbOWfAx49RFrhknORtm3SDmnIaKttsqdovwdkYaJWRIjL6vGccmQp6JXi/snVx55AgyEmNkYFVjlUmG5suSx7zze8JYA3P5k/DfkwNnS9z+qXe+vsLKma5h37ew+f2Oz7/1I+98uKqQlWyVHOOBCnjbmmldiRTZ2GvVc82dDjOulvl1VunLsx6Ccx+F7HHhaVuk4FsHV6m2AtuXxhqYmyb+ixQ/0Sd2NI7bLWacnsqLEwqeEG4fpbEyX1JCO7NWJmRKL8XXjOMhMoR9r82N4w+XSzE2J4XSOssWWF9m8uM4cWophavFgRQOx9G2T+XlseRfEBUDs2+Q9ZsXSCjsjs+9+25ZAGjJvljwjTgCD7kIvv6XbF//Now+AZ4+W8IhK/Il7j1vpvQQ/DnIOovWMmI2NgUOCVDa0qmN1RSLExbE5BNtFf+o3ie9SFv490XOe7ytozIu1Rs+6YvzRZzs538Xlwpo8YU010bW6FlfPCPxy8R/ATKiuHQbpA9v2/aEdEuzD1B+MUI0+z4l7AGS46LZX20Je5MMzUt9hQgnkOpBLfXiEN2zTDIkpg898HPbWRIX3CXTKRfKi+WFi2R5qCOx2MZ5Mh19ogj71FzJNmgL+5evgBHHSMRLgcN8c9yv5KUSjvS7G9+FrR/CaX8M/nz8dBU8fBi8eCkMPlQibRLS5WXhipawu/ThgY/vC9jhljbBNHunM9vf9+q0+UeyGQe8I/HtwZktTXC/1Vs55hdthX10nGj2TmHvjAY844GubWuI9CkzDkBSXDT5rdZbuTurx0cyOz4XUwvA916R0D+AhX+UYfEfzfV/XPFmbybAQGyYJwUfXA69Yfun8NZN3uX8JTB0tsxvfFf+9HY5utQ8SUg100pAljnaWzPVyaRz5IXkLxqisyx6UGzys64Pvp8deWObnOwoDaW8f/aMPi7sfXFG1ABs+cibA77ModkHE/b1FaJsRLQZxxpjYQd61DgCA3Knty8+EusTjWMnk/veK5JeOgLoc8I+OS6a1c1DRPiEMuS5r1O0Hp4+Cz7+nSwPnCj21LRh3uyMzm56c73Ys1sapQCa0f4yAAAgAElEQVTzP2YED4m07ek3fA5H/VTmX7sGdn/Zdr+8mda1WmDILG/Mtl0s+rT74SdLYfBUWY5OgNt3iKPsijckD3v6MDHlhFJJ6fXr2/oonPdXuEaiZqI66Ni6oryVnWbfCIdf691mR5J4Xgj9BGdEDcDzF8ATp4hW78yl41fYW9E91dbgo0jW7J1mHPC2GSB1cPsRsbawt6N37OygEeTH6YNmnChKG6Ng4Ehvncu+zo5Fonn4sz9/7uhCumLEuQQi9CutMDJntMxnf4Yv/gbnPCLLNYUSNTPXT9e9pUlypow+SYTxCXdJoY6YeHF8nvk3eHCchKQ5Y5cP+57Y429YJFkGQXwH2eO9ycGSssVJdoYjP03aUHlZVO/z3oc/ynfC6v/J54gb227bvtB64YRY2PuHH0h8vZ2L3sYeSdnXzTi+OM04zl7fX33GF/iz2duafbVl7onU0EuQtqoorxmn2mGiShrYNjUyeO+lpV4Ef8lm8e2kHYR5NMz0Qc0+hvrmVnSSNQKwt7PzCyncHIynzxRt2tYq9q2Gpf+R7vW6N7wmE3ez11kUE+89vnSrN+Z9l6WRL36447bt/kpeGDOu8p7z1nVw8wo4/3F56I+9TbZNcgw9t9sz+ND2zit7iL0/551tMinrICLH+YKz7ag7FokJad2b4hS2exQdERPfXtCDN6a6v5lx4tJE8M1Ng20fB97PX5x9nK9mH6GDqkCey4QM/5p9UrZ/Mw54n7fiTTBgrPQOI4Q+p9knxcmX2xKfRUyZnyIMvQV3K7x2rTe8754y/w+OM7y0ZJM4XN++2bsuKg4uf00KT48+0bvezmg4+DAJLawrk1qhthO3JIQiFaVbZRoofBHg6FvEFOPssmeODLy/HZ/d6ids1n5JlGySilL+KFgF3zwvf9T6comNj02SFyJIFFD2eIkaCgeDDg3PeXoLtnYOolA4ueAJUTrA//drC0Q793skm3FAepa2Zu90PtuK0uWvec05vsK+ZFPovcduos8J++Q4uaWmuAxi6jrIux3JlO/0CnqQKBandmxjj+gD0Vw/f1BGcrpbJGHXKfeKyeOWtW27nnaSr5zJIuxLt4q24jsAysbtbp+lsWy7dFWD5YNRyvunvmqemFGChXva5h5/OdNT80QbLA7yEl/zigx6+c6f4Y3r4eHpcLejsETBN3DoJYGPD5XoeGhpiPy0B+Em3jGqtnijfNfXfybO87gU+QQKjLBNHbawj2QHLYjd3lezP93RaxxzsnfevrfmOok0qsj3ZgmNEPqesI+XW2qIzSCprgxaWyRr39hTpJhEb8EuFnHFG/DubSLEJ57d3uxRsdM7/8l9Mj3tD5B7WNv9fEMrT/qtdEenXACrnhf/RvJAbxIosPJ9WJpKY1V7O2XZdik7F2qq3pHHBNbIbQZao6BbG9tvU0q08uKNgY+vLZH7sHsPurW9GSwcdtSbV7St5NRfcGr2tcXye+VM8q7zrV3rxCPsrRd5b9DsbWWqukAiymYHiODyaPZ1ULoF0BHlnIUQbfZKqe8opTYppbYqpe7ws324UupjpdRqpdRCpVQQ71nXkmRp9pWp4wANm96VUL4P7+mpJh0YlZawTx8Ox9wqg6C2+thIG6vhuQtkfoClEacPC820kJAOJ9wp9mgVJZq9s5cAbXPG+Es/UbrNO+AkXCSkw7G3w5Vv+d+ePSF4HdT6Mum12HHS4I0SmXi2NT3z4NuZNsSb/74/4RT20LnfP8qqTOXR7CNc2Ptq9sHGZNj30lTjHXeS1cuEvVIqCngEOB2YBFymlJrks9uDwDNa60OBe4E/hruhoZJiCfuCnBPE/uuMIQ9WAT7SsDX7lMGSdCkhA1a/1HafDdYAJVe01+F4ziOdK4oRFSNJn4rWSyw5wBjrXCfe7X1gfXOEuFtlxGQw+/uBcuJdUh7OH9njJUIoUO6jujKrcLZD2K97QxyLFzwBN60Qu73hwHAmRwMZF9EZYhIcmn0EO2hBYu2rCyTQoWpfcHOlrdk314kyolxt8+dEAKFIhVnAVq31dq11E/AS4Gs8ngR8Ys1/6md7t2Fr9tXNLikz57Qf2kWTI5kdn8Om90TYJ1h5sqNjJbxxl0/seulW0crvLJBRqFe9c2CV6weMgc3vyQArFQWXPg+/2glDZkhqW2gvXKv2ihO1s3/2g8V20gay29eXyfeWkA5nWMne7ARm0bH9UxsPJwej2YOYBm2nZ8SbcSyF4YlTJMVDMM3eacYp2SQKVHRclzexM4Qi7PMA57DFPdY6J98C51vz5wEpSqkB9AC2g7amsaX9CElbW45UaktlANSLl4pJJdXxNefNkPY7X1hl28RsEx0nD9uBCHpoq4EkD5Tz2Q5ce+or7AvXWsd2t7C3slEGstvbmj3ArOvEpwAmbUa4iPfR7DurvTqd873BjOMkZXDgfZ1mnOLN3qypEUS44uxvA45TSn0DHAfsBdoNc1RKXa+UWq6UWl5cXOy7OSy0EfZJA+DiZ+HU+2VjdYRr9ls/9M5vXwhpDmE/6niZbnjHu650m/8Y8M6S7XgwfbvW/oT9pvfhpctkPhzX7wxpw2R0bYkfzb6mWMxNGSO86+yeQLA/qiF0fDX7TptxHALeN1Y90hh7atvl1CDPUKxD2JdtC14ApYcIRdjvBZzhC0OsdR601gVa6/O11tOAu6x1PkZe0Fo/rrWeqbWemZ2d7bs5LNhmnNrGFlkx6WyYeqnMR7rNfvMH3nl3i5gebHImiaZvJwfTWgYXhUOznnyed953MJMdgVPv+DntNAsQWhm+cOJySZSDP83e/m6cDmpbE+0K30J/xNdm39nQUzvK6rQ/RtSAI7+kDobTHSO4g2r21v+mIl/MmxE0ctYmFGG/DBirlBqplIoFLgXa1JdTSmUpZY/S4dfAk+FtZujERruIjXZR0+joWPgOeIhUdi2WGHB7kFKqj7UsZbBEBbQ0StWmpurw2MzjU2WACLQX9nZGP6dmb6f4dUV3zhkcLrLGeSMenCz5l2ieTgfssCNlOv707mlbX8dXQHc2fa/9H+wtZjWn3T1Ym6NjJR3J0n/LcoSUInTS4T9Va90C3AR8AGwAXtZar1NK3auUsmLZOB7YpJTaDOQA93dRe0MiOS6amkZHcq/oeEAFKS7Qw2z9WJK21RRBzhSv9uSb/yVlkOzz+4HeLJbhspnb5hp/o2ETMtpq9nYGwJt7KNFc0kBJdfzK1W1j3cu2S9fbaVeecTXcusGbg8fQs9hRVr5jNiKVaEdakY7MTk6HcwQOtgtpUJXWej4w32fdPY75V4FXw9u0AycpLopap2Zvp6QNVDasJ6krg+fO9y7nTJJ8ONDegZWc074sX7hs5nkzJLZ9+NHtt8WntQ29rN4naV57Ki+M7YBd9wZMv9KbBqK2uH02QqXamsMMPcuZf5cX74gDDCbobpzCvsN9EwArSVxS15ipD4Y+lwgNJBladUNL25Uxid7RoJHEep/BQwMnw5E/lvkhM9tuGzSlfVHjcKbYHXW8/5wmvgUr6kp7tpvqLJdnF05vqpWeWwR2nw0O4lNhzs87Ti8dKdjCPiqEMEpnqvDeqtn3NpLjorwOWpvYxMjU7J1Fq0FMNamD/acUnvEDyQW/71sRanuWhy+hVzDi09uGrdaXe6NcegJnSJydjM0elRmBGlWf47zHpUDNcb/q6ZZ0PbbNPpSYeVu+nDzXaxaNIPqosI+muMYnt0pscuQ5aKv2ickmZwoUrRVbfTCHl1Ki3Q+aIsvB8pCEk/g0bw1WgLry9jHI3YnTvFVq1SywR2UaYd/1TL2kfbnCvoqt2Yci7FvqZZo7vevacxD0STNOZlIcpTU+KXIj0Yyz/ElAexO0+Ya1RQpOM05rs0QB9aTmMmw2TDhTBq600+yNGccQRux0xtFBMrX6YldbizD6pLDPSomltKYJbRfkgMgz4zTXS0WoEcd4U6X6VlWKFOy6o263NyonsQc1+/g0Sekw+gTR7LU2ZhxD1xAVK9NQNPuz/ymmrQiNNOqTZpzs5DiaWt1U1beQlmjZtGOS/OdI7ynKd4pDZ8bVIjjv3Bc8z3tPEp8GaNHoK3bJOt+ol55gwBjprdUUeR21Rtgbwonb8v2FIuynX9G1bTlI+qZmnyw/TBu7fWxiZNns7dJ6du6W2MTOD1DpLmxNpaFSBn5B4KyU3Yk9xqB0q7zIY1Mi94Vp6J3Yo2anRVYhkgOhT2r2uenyh88vr2PMQCvXS0yECfsdn1lpULs5t8yBYOdDqa8Qh3LW+MgILbOTcJVuFTOOsdcbwk3yQLirKOIyWB4IfVKznzBYRrqtL6jyroxN7rkRtE21kjzMLgje0gSrXoApF0ZkiFY7bGFfVwq7voIRc3q2PTapQyT+uXSbJeyNCcfQBcTER26vuxP0SWGfGh9DXnoCm4scic9sM47TaduVFG+GFy6FxhrJ2fLiJbDkUdm2c5GU+ZtyfvBzRAq2sN+5SOz2kSLsXS4ZQVy6Tcw4RtgbDAHpk8IeYHBaPPurHDb7mERAS5Ho7uCNG6QgSP4SiaEH2GDlj9v4rrRn1PHd05aDJd6y2W/7VKZDZ/dcW3zJHCnObmPGMRiC0meF/cDUOIqqHYLdWUmmO7AH+1TkQ8Eqmd+zTAqHL38Chhzee5yJtma/f4NUsoqk3PDJAyVXT11JZPgRDIYIpe8K+5R4ittp9rTPLdMVVO2DRmsQ0oK7pVbrYd8H7YZP7pP1vlW0Ipm4VEDJCMGUQT2T1jgQyTlS5k67jRnHYAhCBP1rw0tOajzVjS1UNVjJiZwFgbuaLQtketJvpdrUgLFwyr1eYTRgLEw8s+vbES5cLu/o3kjLQ+50cBszjsEQkD4r7A8dIqaHFbusohvdZcb58Lcw7xZxHM75Ofx4Cdy0TASRnTiqtSn4OSKRBMuUEwmDqZy4HNHDRrM3GALSZ4X99GEZxEQplmwvlRW2Gacr8+Ps3wiL/w6Tz4cfvC/hWvYHpAD2WQ/BJc92XRu6Cttu35NpEvwx/UrvvBH2BkNA+qywT4iNYuqQdJZsL5MVnoLAYdbsV70Ab/1E5ncukulJ90BKAA14xtURmygpKHZETk9mu/RHdJx3cFXigJ5ti8EQwfTJEbQ2M0Zk8OQXO2hsaSXOLggcbgftmz+S6a6vpKp8QmZ4C4pECnZCqEgcBPb9V2DDO0azNxiC0Gc1e4BD8tJobtX86tXVrC2xHLUNFdBYHfzAUGmu987XWz2ItLw+MdquHXbPyDbnRBKZoyRNdF/83g2GMNHnhT3Am6sKuPwZq/jGu7+APw4JclQn2GsV3L7gCfj5eph0LpzzSHjOHWmkDZWpsxatwWDoNfRpYT8s01vRqJ4uSGS0+yuZjj5RNN+Ln+6d9vhQOPxaMVFNOrenW2IwGA6APi3slVJcccRwABqJQasw3+7ur6QWa6RFqHQFA0bDr3Z40wobDIZeRZ8W9gD3nTuF9352DKBoiXKkJ2htDnhMSLhbIX8pDDvy4M5jMBgM3UCfF/YAA5IlkqRVxXpXHmxu+6J1krnSCHuDwdAL6BfCPiEmCoDYFkd+e2fahNoSeOI0b/KyUNi9RKaRULHJYDAYOqBfCfuWNpq9Q9iv/p+kIl7899BPuu1jSM3rmzH1BoOhz9EvhH10lIvYKBevj/uzd6VzcFVNkUxdIY4xq9oHm9+HqZeZ2G6DwdAr6BfCHiR9woaEGXDFm7LCacbZ961My3eFdrKnz5LpqOPC10CDwWDoQvqNsE+MjaK+udWb/bLR0uzryrzFRUq3dHwid6t3v+wJ4W+owWAwdAH9RtgnxERR19QqSbOi4sQMs+tLeGS2ROYMO1KqSjlTIPijrsw7b3KxGAyGXkJIwl4p9R2l1Cal1Fal1B1+tg9TSn2qlPpGKbVaKXVG+Jt6cCTERtHQ3CoDoA69CL59Ed67HaJi4PpPZYQoumNTjm3fv+hpY683GAy9hg6FvVIqCngEOB2YBFymlJrks9tvgJe11tOAS4FHw93Qg8Wj2QPM/pHY7AvXwMjjYNAhElkDULU3+IlsYR9pRTwMBoMhCKFo9rOArVrr7VrrJuAl4ByffTRg1a0jDSgIXxPDQ4JtswcYNAWmXCjz8VazU60i2tX7/J/guQvg8ROgeKO1f27XNdZgMBjCTCixhnlAvmN5DzDbZ5+5wAKl1M1AEnByWFoXRhJioiiudhQgP/Nv4G72VjpKsYR9lZ/3VGsLbP3I2r5XUuqa+HqDwdCLCJeD9jLgKa31EOAM4Fml2mcdU0pdr5RarpRaXlxcHKZLh0aiU7MH0egvfgZyJstydJw4XP2ZcSocdvyaIhh3urHXGwyGXkUown4vMNSxPMRa5+Qa4GUArfVXQDyQ5XsirfXjWuuZWuuZ2dndG8mSEOuw2QciZbB/zX7/hrbL478TvoYZDAZDNxCKsF8GjFVKjVRKxSIO2Ld99tkNnASglJqICPvuVd07ICEmmoaOhH1qnoyObWmU4uE2Bd+AioJ0SZdskp8ZDIbeRofCXmvdAtwEfABsQKJu1iml7lVKnW3t9gvgOqXUt8CLwNVaa91VjT4QEmJd1DW3ErRZqYPFjPPBXfDobKjcI+sLVsLASXD9QrhpuYRrGgwGQy8ipGQwWuv5wHyfdfc45tcDR4e3aeElMTaaVremuVUTGx3A3p6aK7Vk174myy9fCUf8WDT7iWdJjH5/KFRiMBj6HCFm/ur9xFuZL+ubWomNDtChsWPt7eLhe1fAa9fIfO70Lm6hwWAwdB39Jl1CYqwl7JuD2O3t8Et/5E4Lc4sMBoOh++g3wt7OaV/X1BJ4J1uzB5h1g3c+Ol5s9gaDwdBL6T/CPhTN3jlQ6uifwngrxc+UCyE61v8xBoPB0AvoNzb7BIfNPiAx8d751Dw49fei1Z92fxe3zmAwGLqWfiPsQ7LZgyRGK98hI2QHjIaL/q8bWmcwGAxdS78R9vEem30Hwv7Kt0wqBIPB0OfoNzZ7W7Nv6EizN4LeYDD0QfqNsLcdtB1q9gaDwdAH6TfCPjFGLFZG2BsMhv5IvxH28bFyqy8t3c39767v4dYYDAZD99JvhH1slIsol2LL/hr+s2hHTzenx2hqcfPbt9ayfGdZxzsbDIY+Q78R9kopT6x9f+bNVXt5+qtdPPTxlp5uisFg6Eb6jbAHr5MWwO2OqAzM7dBa89LS3ZTUNHa8c4iU1zZx+6urAdqWaDQYDH2efiXsU+O9wwo6HFzVw+SX1XPH62u45N9fhe2cD3/i1ea37q+hobmVF77ezf7qhrBdw2AwRCb9StjHRns1+0iPytleUgPAtuJaWg+iF1Lb2MKvXl3Nyt3lfLGlBIDpw9JpcWse+XQrd76xhgfe3xSWNhsMhsilXwn7qUPSPPNBs1/2EFprbn15FT95YSW7Sus861fuLg963Osr9/DQR/5t8B+uL+J/y/M5/9Ev2bK/hjvPmMBDl0q65n98shWAivrmMN2BwWCIVPqVsL/nrEnceNxoIDI1+x0ltby+ci/vrt7H+oIqADISY7hv3vqg2v2tL3/L3z7a7BkdXNPYwrqCSgDeX1vYZt+jx2QxJCOhjbN6W3FNuG/FYDBEGP1K2CfGRnPEKCkrGInCfpkjHHLB+kImDU7l1lPHs3pPJRv2Vfk9xpnF0+4B3P3mWr778Be8v7aQzzYXc8Yhgzz7TByUilKKzCRJ2TwkI4GdJbXt0khorbnrjTVc/8xy/v3ZtrDdoyFyuOmFlTzz1c6eboahm+hXwh4gKc4eSRt5ZpylO7zmmvK6ZkZmJXHc2GwAVuVX+D2msMrrXP14w35+8sJK3vhmLwA3PreC+uZWLpo5FID0xBhcLsn9Myk3FYCjR2fh1uKwdbKvsoHnv97NgvVF/PG9jWG6Q0OkUNvYwrzV+7jnrXU93RRDN9HvhH1GYgwA+6siL/Rw+a4yTp6Y46mROyIrkaGZCWQmxQYU9s7QzCe+2MG7q/cBcNbUXM/6I0cNYN7Nc/jw58d51t1/3hSuPmoEVx89AoCNhdUAVNY18+H6ooDXM/QNtjhe7puLqnuwJYbuot8J+5FZySTGRrF6T1thFrSoSReiteaKJ77mrws2sau0jtkjM0myxgMMH5CEUoppQ9P5JoCTtsSKlz9mbJZn3YkTBnLbqeMA+O6hg4mPiWJKXhrZKXGefQamxDP37MmMy0khLtrFRstM9I9PtnDdM8uZ+3ZbjU/ryB6XYOgcmwq9ZsGPN+zvwZYYuot+k8/eJsqlOCQvjVV7Kml1a1rcbrYX13L6Q4v49xUzOG3yoI5P4qCpxU1jSysp8TEH1J7VeypZtKWERXZY5PAMJuWmsnhrKXnpCQAcNjSdjzfup7K+mbQE73Xcbs2/P98OwJ1nTGTx1hLOnz7EY4//9renEhsV/H0e5VKMzUlm2a5ydpTUsmyXvFT2+wy6qmlsOeB7NEQeGwurSYiJwqWgqMqMs+gP9DvNHkR4biio4hcvr+KCf33pMVm8821Bp8/1g6eWcsjcBX63VTV0HNI4b3Xba04cnMJfLjqMq48awcwRGQBMGyZT397IRxu85paxA5O59phRHkEPkJYQ02bUcCDG56TybX4FJzy4kHV7Kz3rEx3HltU2dXgeQ+9hc1E143KSGZQWb4R9P6HfCvumVjdvripg7d4qnlq8ExAtvTM0trSyeGspAKU+aQ1e+Ho3h85dwM6S2oDHu92aeZaNHUTLToyNZlCamFjirEFghw5NQyn4ZndbYb/dOvdHtx5LdAcafDAmDErxzLc4Qjx/cep4Ty2XYMJea01za+e+O0PPsqmwmnE5KUbY9yP6pbCfOjS9zfImy0FV2kntddHmEs/8ffPW883ucm54djm/e2edx+b96ab9LNtZxhVPfN3OL7Bydzn7Khu49HCJlnGmc3CSGh/D6Ozkdk7TveX1pCXEMGZgit/jQmW8Q9grhcd8dPqUQbz546OBwMK+vqmVe95ax9i73uv0y9LQM5TUNFJS08T4QSnkpSewq7TO+GT6Af3OZg8wOC3eMz92YLInMmFfRX2nzvPO6gJS4qI5ZXIOr6/cy5ur2puBnv5yJ5lJsazcXcG81QWeMEgQs1FctIvLjxjOS8vymTAoNeC1pll2e601ylK3CyrqybUE88Hg1OwnDErlscunU1rbRG56gmcwlz9hr7XmvEcXeyJ5Pt20v9M+D0P3s8n6vSYMSiUmysXLy/ewr7IhLM+SIXLpl5q9UortfziD+86ZzP9uOJK/XjyV86fnUVTdGHIemvqmVj5aX8R3Dx3MXy8+jP9cObPdPo9dPoOdpXWstMwv3zps7lpr5q8t5MQJA5mcm8rvzp7MP743LeD1pg3LoKy2ic1F3pC5vRX15KXHBzwmVLJT4jwCf9aIDIYPSGK65SfIsHwA/oT9yt0VHkEP8NqKPQfdFkPXYwv78YNSmJInKUTWOnw1hr5JvxT2AC6X4oojR5CZFMv504cwfVgGrW4dcurfxVtLqG1q5buHDgbwjMydODiVpXeexLyb53Da5ByOGj3Ac8y2/V77fVV9C8XVjcwYnoFSiquOGkFWchyBOG2yxN+/sjzfs66got5jcjkYlFI8e81s4qJdnDQxp822pNgoYqNdlNW1F/YLN0nIXkJMFErBgvVFJmY7wlm7t5J7560nKTaK7JQ4Jg1OxaWMsO8P9Fth70uupSEXVIZmyvlyWymx0S4OHyFCPiU+htVzT+X5a2czMDWeKXlpKKX4z5UzeeKqmZw/La9NDpriGnGKOWPfgzEgOY6hGQnstUxN3+ZXUNXQEraud3ZKHGt/dxrHjstus14pRWZiLGU17YX94q0lTBuWzob7vsNTP5gF4MmXb+g+3G7NF1tKaGl1U1HXFNT+/n9WMMIhVlLAhNgoxg5MYY0R9n0eI+wtBqeJ0NxXEVpkwlfbS5kxLIN4R0Kx1PiYNqGPIOkZTpqYw5icZPZXN1JthWPacezZQbR5X7JT4iipaaS+qZVzHlkMEFY7a0yAiJ7MpFjKfTT72sYWVu+p5MhR0nM5blw23589jFX5FVSaLJrdytc7yrj8ia/57sNfcNi9HwZNgWCPuLYznwJMzktlbUEVWuuDSqdtiGxCEvZKqe8opTYppbYqpe7ws/1vSqlV1mezUqrXjbXPtYV9CJr9jc+uYMO+Ko50mGg6YnR2MgDbi8WUY4dRhqrZy77xFFc3eqKHILzCPhCZSbHtIpWW7yqnxa05YpT3Ozhh/EAAbn/12y5vk8FLfpmkw7afi2eX7Ao4xmP9vioumD6EnFSvr2fqkHSKqxsZ+ev5/PSlb7q+wYYeoUNhr5SKAh4BTgcmAZcppSY599Fa/1xrfZjW+jDgH8DrXdHYriQ1IZrE2CgKOtDsm1vdvL9O0gZ3RtiPGSjCfvXeSp79aicPfLCJ2CgXg9JCd7BmJ8dRXN3Ish3e7JjhsNl3RGZSLOU+wn7lrnKUwjPwC7z3+MG6InaVBh5fYDg4Kuua2yTy21tRj1Lw0KWHecJ47UI1TvZXN1Bc3cjk3LZRX6dO9vpp3l29z4Rh9lFC0exnAVu11tu11k3AS8A5Qfa/DHgxHI3rTpRSDE6LZ19lPVprWgIMEnI6cKcOSfe7jz9GDEgiyqW4+8213P3WOnLT4vno1uM6lYJg6tA0aptauX/+Bs+6zvQMDhRfzX5TYTUPfbwFrSVttM2wzETOPUwSsP130Q7ufWc9202u/LAz9d4FnPfIl57lgop6spPjOOewPH5/7hRS4qP5bFNxu+PsGgmTfIT94LQEHr5sGkMzRXHIL+tcCLKhdxCKsM8D8h3Le6x17VBKDQdGAp8cfNO6n9z0BAoqG3jiix2Mues9ahq92tOCdYWs3lPhSSn8j8umebJThkKUSzE6O8mz/PBl0xg2ILFT7Tvr0FzPGIEjRmVy43GjibJSFnclmUmxVDe0eEbJvrw8325vh/sAACAASURBVO9+Lpfi75dO4+ypuTy7ZBdPLt7BhY+Fr4auwZuae1NRtacGgTNGPjrKxZwxWXy+pbidhr7eSnY3cXD78RxnT83ln5dNB/AUvjH0LcLtoL0UeFVr7TeFpFLqeqXUcqXU8uLi9ppHTzM4LZ59FfWecn12ZsC6phauf3YFZ/9zMXvLReuxbfCd4bHLZwAQH+NiphXF0xlcLsUTVx3OlLxUHrhwKnecPqHT5zgQ7Fh725QTHSUvmB8ePdLv/udP9+oCkVg3oCdobnWH5bvYsM/rr5lw9/s0t7rbheAeOy6bfZUNnhoFWmv2VdazrqCKoZkJbZLpORk/KIUol2LN3kreXb3POGv7GKEI+73AUMfyEGudPy4liAlHa/241nqm1npmdnZ2oN16jEFpCRTXNHo0+vXWH8vOfyPzJaTERXvs051hVHYyfzz/EF654agDbuOk3FTm3XwMQzM71ys4GAbYA6usiJy95fWMGJDIPWdN8rv/jOFeO35Lq8bdC4XGS0t3s6e8ruMdQ6C6oZlp937IpHs+6LQ9/LPNxcy6/yO+3Co2+PU+Fcs+3bifvRX1ntBhgDljJN31ku3y3D739W6O/OMnvLt6H5P8aPU28TFRjB2YzKMLt7UpgmPoG4Qi7JcBY5VSI5VSsYhAf9t3J6XUBCAD6LX99ty0eLTGo9Gs21uJ261Z7igX+NKyfI4bn90pE46Ty2YN88Q49xYyEi1hb8XaF1U1BHUsp8TH8NQPDuemE8bQ4taU1EZeoRh/vPD1btburaSwsoE7Xl/Djc+tOKjzNbW4Ka5u5A/zN3oUiJ2lob9ASmsauerJpeyvbuT5r3cDsN7HxPLIp1tpbHG30ezz0hOIcimPyfFF61iAybnBnz2nPd8kt+tbdJgbR2vdopS6CfgAiAKe1FqvU0rdCyzXWtuC/1LgJd2LXfmDHX+YnNQ4XlqWz9qCSmKjXMRGuWiyHv5TJuUEOkWfZEByW82+tKaJibmBNUSA48cPpLlVHoXCygYGphx8WoeuZGNhFXe+sQaXgtkjJcoq1DEX/mh1Sw3fV1bsadML/Hp7KSOzkoIc6eUXr3hDWJftLENrzbqCKo4aPYDbThvPy8vyeWmZ+E+Os8JeQcx9WcmxnmpsTt9TMM0eYObwTF5fKRq9M7Gd1poVu8o9I767k9dX7iEzKZbjHfdo6Dwhqada6/la63Fa69Fa6/utdfc4BD1a67la63Yx+L2JXIe2aif0Wru3ijV7K7n8iOGebceP618PnUezt2z2pbVNHtNOMGxn8r7KyE6hu6Womh8/vxIAt5YBc0CbAXMgkVjnP7rYk1smECt2lTPmrvm8YuUK2rq/hktmDiUrOZavrbDZ1Xsq+PHzKwJmCl26o4yFjoia/dWNLNpSwuo9lUzOTWX6sAyumSM+k8m5qe1eIANT4imuaaSpxU2+wxzlG4njy5lTBzPCChyoqPPG6n+8YT8XPvYVzy3ZFfT4g2VjYRU3PLvc43zeVFjNrS9/y3XPLO/S6/YHzAhaB84/zPAB3vnmVs1RowfwwnWz+f25U0hL7F8Vm9Kt+y2taaKuqYXK+mYGJHUc8mmbeiI5X7rWmjMeXsT24lrG53izf2Ylx1LskxjvrVV7Wbm7gnvnBR6hqrXm3nfW4du/HZAcy8zhmay0ykv+6rU1zF9TyNoAkS+/e2cdeekJzP/pMbxw7WwArnxyKQAnW/mLxuakcMNxo7jl5HHtjh+YEsfCTcWc/tDnaA3XzBnJdceMbJPx1R+p8TEs/OUJpMRFU1EvL/fdpXU8/7UI+XfX7At2+EFzzVPL+WBdEeusMNGPNhQBeHqJhgOnX6Y4DkR0lIuvfn0ijc3udkP+ZwzPICMplqNGZwU4uu8SE+UiLSGGV1fs4aGPtwCQmdyxZp+ZGEtMlGJjYTW3vryK20+b0KlBZN3BV9tKaW7VjMxK4qkfHs6/P9tOTWMLh4/I4FevraGgot7jDLcF9Te7K6htbCEprv3fZ97qfXy7p5LU+Gjc2mtCyUyKJS0hhvfXFVJa00ic5fPZUiTlAT/fXExOajxZyXEMy0xkXUEVd585iUm5qWitiYt20dji5qYTxjDbMWr516dP9HtfIyzFZVtxLXedMZEfzhnZqTDd9KQYKuqa2VlSy2l//5xGqweyZHsZJTWNQZP2HShut/bkfsovq2PG8Aw+2+zt3VTWNXsUrVdX7CEu2sVZU3PD3o6+ihH2Ptg5cgBuO3UcDy7YzJS8VE/4YX8lMymWHVZlrAmDUpjpiLgJhMulyEmN5wXLQdjY7OaR70/v0nZ2lue/3k16Ygzv/ewY4mOimHv2ZEDMKADbims8wn7DvmoyEmMor2tm0ZYSvjNlEA3NrRRWNniE65/e28j4nBTeuXkOMVGKX722mpeX7yErOY4UqzhNfnk9ttzdVlzLowu3scvhuD1lUg5KwQnjJWJNKcVNJ4zh2z0VXHfsqJDua0iGPMe/PG18yMc4GZ+TyjvfFngich648FAaW9z85s21XSbsV1gvU4AP1hXS0NzKsp1ljBmYzNb9NeworWVqQhofri/iNsufcczYLNIT+/d/M1SMGScI3589nFtOHus3V31/w07wNjQzgfdvOdbvwBx/OM0GoeQd6k601izaUszpUwa1s8/bJj07l1FNYws7S2s9abH//tFmahpbmPv2Oo5/cCGfbS4mv6yOvRX1nDc9j9hoF0opZg6X8RSZSbEeAVlS3cgea7zG7tI6T24bmw/XF3HjcaMZ5RjLcfNJY/nvVYcHjJH35bJZw7jt1HEBx0J0xB/Om9KmROUF04cwyvpOymu7JtHdB2sLiY12MWJAIu+tLeSO19egNVx11AgALnrsSy567Cuuf9YbJfXVttIAZzP4YoR9EDKSYrnl5HFttP3+iu2kHdbJ+P5Bju8ukhy1La1u7n5rLVUNLX4rhGVZZqp7562npKaRTYVVaA2H5qXxwIWHsrGwmvmr97HIykHz5bYST3pnZ+6Z7xwyiGvnjOTwEZme1Bb55XWerKfvryvErb3Xszl/mt9B6iETHxPFTSeODangvD8Gpsaz8LbjPcsul/Jo0JX1XVN8fltxDeNykj0+CRDN/Tzru2hu1SzfVe5ZD17TmqFjjBnHEBKD0kRQDc3onLAf6MjdU1TVQEur+6CKo4eLFbvKeW6JmJf89VKc4YXrC6rYbWnfE3NTybSE3uaiao+N+fWVez2Dx452+HVS42P4zZky+MzlkuPsWsLJcdEem/4rNx7FXxZsYk95PavyKw5o0F64GZGVxLyb53g0fNtRX14Xfs2+tKaRXaV1jMpO8tz7KZNy/Paqrz5qBHPPnsyJDy6kIIIUiEin5/91hl5BdrKYY1JDNCPYHDsum3E5yZw/LQ+3hqIQK4F1NYut7v/Dl03j8BH+/Q/zbp4DwD8/2cq6gipiohSDU+OJj3ERE6X4eKNU6vp/FxxKQ1MrpbVN3HDsKFwBHKFx0VGkxEfzllWr+JaTxwIy4nVkVhL//N50Xr7hSFbPPbXbY9kDMSUvjcOGSsI/W9hXhFnY1ze1ctrfP2d7SS3piV5zlx1+6eTT2473+FVyUuMpNMI+ZIywN4REXIw8Kp1NvHbcuGwW/Pw4zrayYXa2qHu4eOyzbdw3b71nefHWEqYOTefsqbkBBattjlm6s4wXl+4mJzUel0uhlCI1PoYdJbUkxERx3vQ8/nX5DGKjXJ6aroE4wTEw6JzD8lh0+wk8fuUMz7rYaBepnciE2p0kxEiJyoowm3EWbtpPiaMS2vThGUS7FDccO9qz7lBr1PnQDK9ZcHCaEfadwQh7Q0hcNmsYZ0/N5do5B+bws7MyOrvdW4qquf6Z5WwsrAp0WNj403sbeeKLHbjdmuqGZlblVzBnTPB6BEopXrh2tic1hjO9tR1ZM314OjFRLuaMzWLlPadwplWTOBB/PP8Qz3xWcixDMxPbpImOZJRSpCfEUBGig3ZXaS2/e2cdzy7Z1WYUry8b9lUR5VLcf94Ubj9tPJlJsWz9wxnMGes1hz37w9m897Nj2pgAB6XFU1TVwOKtJRx+/0fc8Oxy1uwxGTsD0TueMkOPk5YQw8OXTet4xwB4RtNW1PPu6n288c1ez4CZmsYWthfX8t1DB3P3mf6Tq63ZU8nyXWX84ACiS5w5XraX1LCrtI5Wt25jWw/EUWOyWPGbkzlk7gKPOcPJrBHeF0ayn7h7X5Liohlg1QeIFFNNZ8hIjA1Zs7/zjTWeJII1DS386PjRfvdbv6+aUVlJfH/2cL/bAdISY9oNZhyUFk+LW/PWqr0UVzfywboiKuqa+d8NR4Z4N/0LI+wN3UJKfAwpcdG8uHR3u2RgX1r28ye+2BFQ2J/1zy8AOOOQwW1K6oXCt/neKpmr8ivZVFhFbJSL6SGMFbDbvviOE0lyRLbY93DEqM6nqv7ktuN7bZKxNGucQUe0ujXf7K7giiOG88qK/HbZOp1sLKxi2rDQfgsng6zn4JvdFeSlJzBnTBb/W57PSX9ZyO/OntKmZ2AwZhxDNzI4PZ6dpXWMGZjML08bD8DRPqYUf+mQqx31VP2V2/NHdUMzjS3i4Fuwvohoy9fwbX4FS7aXcdiw9Hax9cHIS09oM3jnl6eN5+SJOcwa2Xlhn5YQ0yWDkrqDjMQYKkMQ9rtKa6lramXq0HTmjMlm5a5yv3mAqhqa2VNez4RBKX7OEhx7NPaW/TUMSotn9EDvqOG/frip0+fr6xhhb+g27CH335s1zKMlDx+QxFxHXvw/v7+x3XFb9ntLG67Z27FNVmvN0X/6hEsfX8KXW0t4/PPtDEiO5chRA3h2yS7W7K0MaQRwMH5ywpj/396Zh0dVJY37rYTsK4SQsAhBZA2rBBRRUBwVZlBc0Lh9svhDGUdm/HCZwRHFZXBG+VzwxzjyzSgqOIIoLggjbmyK7EGQNUDEsIYACYHsOd8f53anE7KT7r6dPu/z9JO+t8/tW+muW123Tp0q/jkmxSdDMedDbFjdwjiONRXtmodx1yXtOXgqnzdW7D1n3PdWmKdf+7q3+HTgWnojMSaU4cmtub5PG4Z0iWfTgVOmD3IljLE3eAyHWRzRK5FRfdtyZdd4Jg27iLGDO9LHioe/sXIfpwuKGT9nvXPiNv2oNvYtI4OdfVRrYv/xM+QWlLD5wCmmfrINgLsv6eA8B8CInjVPpBqqJjY8qE6plw5jnxgdylXdWvGbXq157dv0c2pOLdt+hJiwIAY2oHNbS5difD3bxNA+LpzX7ujHC7f0JkDggw2Z9X7PpoyJ2Rs8xux7Ukj75ZRzRfKccQOdrz07KpkJ72zgaG4h//+bdL7ZeYyM42f45pEr2XPsNCHNAhjeM5FF1uKl6nLZAVanl4d69mad4aFfdebBYReReTKf0rIy7rykQ51ryhsqEh0WRGFJGQXFpTWGwf7+rW7t6fC+R/RK5POthzmaW+As+aCU4tudx7i6W6sGLbRz1YFBncrDgYkxoXSKj3S2ZTRojGdv8BhdEqK4LeWCKl/r3S6W7/44jLiIYN5ekwHAPqvw2u6jeXSKj6R321jOFJWSUcvt+SqXuH6A6LRREeGCFuH8+Tc9jKE/DxyGOje/eu/+TGGJ87tz/CDEhun5Dte7gsyT+Zw8W9ygfsyV6VmpTn/z8GBOnnVPWQdfxRh7g21oFhhA18QoCorLJ/JOnS0i/VgenRMiSW6rL+ifagjllJSW8cPebH7VXS9eGtolvt7ZO4bqcaygrhyOccXhUf/j7vLFYo4fiVMuBtgx/9K9df0nZx387ZZe/HF4t3PuDGLCg2qU0R8xYRyDrWhRqZT0m6v3c/BUPncmtKdzqyiCAoVth3KqrWO+JTOH04Ul3NivLW1jw7ilfztPiO03xFQy9iNeXUX3xCheSu3rHLP7qO7k1SWhvL6Po9SCqwH+97oDtIoKqbV7Vk2kDmhf5f7m4UGszyhAKeV3k+jVYTx7g61wtDvs2DKCKzq3ZOY3OvbbuVUkwc2051/TJO2avccR0cXInh7Vk97t6p/lYageh7Ef/Y81HDyVz47DuXxk1bxXSjF75V6W78oiuFlAhW5vMZWMfW5BMWv2ZnNL/3aENGtYZc6aUEqHjN76LqPR39tXMcbeYCtaWBkW8VEh3DGw3Gsb0kU38ujZJoZtB3Oorq/93qwztIkJ8/tmM+7CtZ7+cy61hpRS7M06w/QlO/l862E6xUdWqKMUGdyMACmP2e86cpqSMtWgdQp1wdFFbL7VkN1gjL3BZsRZdd1DgwKdhuDXvcqbiyS3iebk2eJqS9tmnjzr7NJkaHxcjb2j6idAVl5hhbz2zpVKNAcECDFh5XF0x8KsujSubwgPX9vFWb/o5S93k/Snz9mQccIt5/IVjLE32IpreyTQLTGKUX3a0DIyhC//ewivpJbX5Em2qkr+VM3iqsyT+bSrZ819Q92JDi2f5isqKSPYmhgd+JevmWWlW0LFeL2D2PBgTrmEcfT7uafCZ1RoEDf2bcv+42ecfZNf+WqPW87lKxhjb7AVraJD+c9DQ5wTq50TopxVJwG6J0YT3CzAWU/HlZz8Yg7nFHBhvEmtdBeVs15ucumotelAeQ2izgnnZthEhwU5s3EcqZv17Y9QH2LCgihyqUHkCO2VlJZVGwZsyhhjb/ApwoIDGdolnm9cQggOHOVtHbXPDe7HNSvqk98Ndj6vHMYBiA0Lchr53AJd8jgq1H0JgZX79X625RA7j+TS48kvmPLRVred166Y1EuDz9G+RTjfp59bEG3Bhl+IDGlWZSlig3vo1z6Wbx4eSmJMaIW6/K6ZOA5iw4OcC+Jy84sJDw4kyI0tKl2N/fV92vDZlkMMf2UVUN4a0p8wxt7gc8SGBXGmqFTHjF1CPNsO5jC0SzxRNu301BSJCGnGhfHlXvyTI3uw8eeTVXY0iw0rr6uTk198jufd2DhCRDFhQbw4ujeFxaUs2657KFRVgbOpYytjX1xcTGZmJgUFptWYrxAaGkq7du0ICvKcgXVdoBPv0tA863QhQ7v6ZulgX+I/D13h9JArM/7yjoyvpptZTFgQuQXFlJUpcguK3d5+0bGWqmtiFKFBgbw4ug8n3l7PwVP5zolif8JWxj4zM5OoqCiSkpLMqjcfQClFdnY2mZmZdOzYsHaFDSHGqiv/c/YZp7HPLyrldGFJBeNvcA/dEvWK18svql9zkJjwYJSC0wUl5OaXEB3mXvPTv0NzWseE8sRvulvnD2Lhby9jxhe7+Pvy9FoL6jU1bDVBW1BQQFxcnDH0PoKIEBcX5/E7sViXVZzHcvW5Hf1hW0WZOjieYOezw5kzbkC9jnF8b6fyizzi2beMDGHNlKvPWUUdGx5EmYK8our74jZFbGXsAWPofQxvfF9tXRZN7bLqsBw7rY2+8ew9Q2hQYL3LErvW1cktcH/MvjocHcdOnvFcVcys04WMfG0Vu46c9tg5K1Onb0tEhovILhFJF5E/VTPmNhHZLiI/ich7jSumwVBOp/hIFk+6HNCNSsDVszfG3q445lpOnS0m52yxW3Psa8KxwvrAibO1jKya2Sv3Muj5r+uVq79wYybbDubyxspzu3V5ilqNvYgEArOAEUAP4A4R6VFpTGdgCjBYKZUMPOQGWX2COXPmcOjQIef2qlWrSE5Opm/fvuTn51d5TEZGBj179gRgw4YN/P73v/eInA8++KDbz+MukttEExEcyL4sbeyPWcbeePb2xWHsT54t4nRhSYXVuJ4kyUoLzTjesLaF05fs5HBOQZ0arzvYf1yXfT6eV8Tf/rPTK6mfdfHsBwLpSql9Sqki4H1gVKUxE4BZSqmTAEqpc1e8+AGlpaXnGPt58+YxZcoU0tLSCAurvWZLSkoKM2fOdKeYTQIRoWN8RAXPPjBAaBFuCqDZlcgQbeyP5haglHtXz9ZEQnQIYUGBZGTX37MvcVmRe+hUPtsP5ZLy3Fdsq6U38gkrZLRydxavL9/LzsO1t9dsbOry09oWcC0dlwlcUmlMFwAR+Q4IBKYppf5zPoI9/dlPdeo3Wh96tInmqeuTaxyTkZHB8OHD6d+/P5s2bSI5OZl33nmHNWvW8Mgjj1BSUsKAAQN4/fXXCQkJISkpidTUVL788ksmT57Mhg0buOuuuwgLC+Pee+9lwYIFfPHFFyxdupS5c+fy2GOPsXTpUkSEJ554gtTU1ArnX758OTNmzGDx4sWcOHGC8ePHs2/fPsLDw5k9eza9e/c+R+aysjIuvPBC0tLSiI3Vk1GdO3dm9erVrFu3jueee46ioiLi4uKYN28eCQkJFY4fO3YsI0eOZPTo0QBERkaSl6c9kRdffJEFCxZQWFjITTfdxNNPP93gz7+x6dgyki2Wh3TsdAEtI4P9KrvC1wizmsw7+tO6e4K2OkSEDnHhFTx7pRQnzhQRF1nzneF2FyP99Gc/ERMWzPG8Qr5LP07PtlWv3C4oLmXlnoqLAB13OZ6ksSZomwGdgSuBO4D/FZFzljGKyH0iskFENmRlZTXSqRufXbt28cADD7Bjxw6io6N56aWXGDt2LPPnz2fr1q2UlJTw+uuvO8fHxcWxadMm7r77blJSUpg3bx5paWlMmjSJG264gRdffJF58+bx0UcfkZaWxpYtW/jqq6949NFHOXz4cLVyPPXUU/Tr148ff/yR6dOnc88991Q5LiAggFGjRrFo0SIA1q5dS4cOHUhISODyyy/nhx9+YPPmzdx+++288MILdf4cli1bxp49e1i3bh1paWls3LiRlStX1vl4d9OxZQSZJ89SWFJK1ulCE8KxOeGWsT9qZVB5y7MHHcrZ71Klc+4PP9P/ua9qDe2s219eOXPLLzmctTJ6agrpPP3ZTxSVlBEUWO6IxIR5/g60Lp79QcC1cWg7a58rmcBapVQxsF9EdqON/3rXQUqp2cBsgJSUlBpnN2rzwN3JBRdcwODBus7H3XffzbPPPkvHjh3p0qULAGPGjGHWrFk89JCemqjsnVfH6tWrueOOOwgMDCQhIYGhQ4eyfv36Kr11x/gPP/wQgGHDhpGdnU1ubi7R0ed29klNTeWZZ55h3LhxvP/++06ZMjMzSU1N5fDhwxQVFdUrH37ZsmUsW7aMfv101cm8vDz27NnDkCFD6vwe7uTClhGUKfjlxFmy8gpN2qXNCQoMILhZAIdOOYy995b5dIyP4OudRykuLSMoMID31ungxaYDJ0mqpkfx7JV7WbgxkwtahHH5RS35cvsxDp3S83AHT1U9HwfwXbou2tciIpijuXpuyRuZSHXx7NcDnUWko4gEA7cDn1Ya8zHaq0dEWqLDOvsaUU6PUjmd0BEaqY6ICO9XWRw0aBDp6elkZWXx8ccfc/PNNwMwadIkHnzwQbZu3cobb7xRZU58s2bNKCvTsciysjKKinR8USnlnG9IS0sjPT2de++913P/VC04GofvzTrDsdxC4mu5BTd4n/DgQI54OYwDulBbcani5+wzKKXIslJ3f9iXjVKK6Ut2sGRr+V33yTNFTF+yk91H8xiQ1IKoUF3UzfHDtXJ3VrU9bx2lI/q45PvbMoyjlCoBHgS+AHYAC5RSP4nIMyJygzXsCyBbRLYD3wKPKqXOrUHrIxw4cIA1a9YA8N5775GSkkJGRgbp6bpe97vvvsvQoUOrPDYqKorTp6vOpb3iiiuYP38+paWlZGVlsXLlSgYOHFitHFdccQXz5s0DdCy/ZcuWVXr1oH+gbrrpJiZPnkz37t2Ji4sDICcnh7ZtdRnat99+u8pjk5KS2LhxIwCffvopxcVaaa+77jrefPNNZ/z+4MGDHDtmn7n3zgmRNAsQNv18kuwzRbSKNsbe7kQEN+OIFcbxVp49QBerBPPuo3ms2ZvN8Tzt4Hz+42EO5xQwe+U+Hpi3yTneITPAwKQWRIU0o6i0jKLSMu68pD25BcXM/eHnc85TVqY4klPAgKTm/M9tfZz7vWHs63QfpZRaAiyptO9Jl+cKmGw9fJ6uXbsya9Ysxo8fT48ePZg5cyaXXnopt956q3OCduLEiVUeO3bsWCZOnEhYWJjzB8PBTTfdxJo1a+jTpw8iwgsvvEBiYiIZGRlVvte0adMYP348vXv3Jjw8vFpj7SA1NZUBAwYwZ86cCu9x66230rx5c4YNG8b+/fvPOW7ChAmMGjWKPn36MHz4cOedyrXXXsuOHTsYNGgQoCdu586dS6tWrWqUw1OEB+sKl//56QilZcrE7H0AR9wevBuzv6hVJCIw8+s9HDtdyIUtI5g6sgfj5qxn3tpyo11QXEpoUPndCEC31tEUFJ90bl/TPYEVu7LYeyzvnPNknswnv7iUWy5uR1RoEJOv6cLLX+0mLKjx++7WhniriH9KSorasGFDhX07duyge/fuXpHHQUZGBiNHjmTbtm1elcOX8Ob3NuWjH/m3FW99/a6LGdGrtVfkMNSNq2Ysd6bL7n/+115dMZ/0p8+dz7+aPIRO8ZFc/dIKDmSfpaRM28UVj15Jh7gI3lt7gMcX6Rr4m6dewzc7j/HwB1sA+PK/h/DIB1uICQ/mnfEV79S/3H6UCe9s4MPfXkb/Ds0bRW4R2aiUSqnvcbYrl2Aw1IfWMeVrF0wYx/6UWkb03XsHer00iqPxysKJg7ioVRQiQmrKBU5DDzhj8kdyyidgY8ODKjRdads8jJaRIRy3FvY5yDx5lle+2g1U3abR09iq6qUdSEpKsrVX/9Zbb/Hqq69W2Dd48GBmzZrlJYnsQ3ykycaxO4//ujvbD+VwWaf6Vcx0BzNv78vLt/WpUOPn5ovb8fzSnc7tI7nayDvWBsy99xJEhITocl0LD25Gy8gQth2quLBqxCurOF1YQouIYFv0WDDG3scYN24c48aN87YYtuHuSzvw0pfaezKevf0Z3jOR4T0TvS0GoJMamgVWvLuIjwqhW2IUO62CZQ4jfyS3gL4XxHJ5Z/0jHH5AygAAESlJREFUVbnPcVxkMNl5Rc6yyaeskhCAV+LzVWGMvcGnaRERzP7nf01RaRkhzexxURl8m49/N5idR04z5s11zonZQ6fynRk8wDmeesvIEEqspiyx4cF8uKl8KVJIM3tEy42xN/g8ImIMvaHRCA0KpO8FsbSOCeVwTgFKKQ7nFDCkS3yFcZ89eLkzhTIuUq+IPZ5XSGx4MEu3HqZd8zAyT+Yz+douHv8fqsIYe4PBYKiCxJhQjuQUcLqwhLNFpbSOqTgn1KtdeS0cx4K+43lFRIYUsOHnkzx8TRcmXd3ZozLXhDH2BoPBUAWtY0LZdjDXGcpJjKm+am2c09gXOhuU2C0N2B7BpCbK8uXLGTlyZJ3Hp6WlsWTJktoH1gNfr1tvMHiLxOgwjucVcu3LK63t6rO9WlphnGmfbuepT3+iS0IkF7XyfrqlK8bYu4mSkvr3t3SHsTcYDA2jctim8rYrjlaHx/N0rv11yfbIOHLFvmGcpX+CI1sb9z0Te8GIv9Y4xFHP/tJLL+X7779nwIABjBs3jqeeeopjx445a9X84Q9/oKCggLCwMN566y26du3KnDlz+Oijj8jLy6O0tLRC7ff169dz3333sXDhQhITE5k0aRLbtm2juLiYadOmMWLECJ588kny8/NZvXo1U6ZMOaeapr/WrTcYvMH1fdrw4aZM1lpljWsqxxFYqY9Ccpuqa9t7E+PZV0F6ejoPP/wwO3fuZOfOnbz33nusXr2aGTNmMH36dLp168aqVavYvHkzzzzzDI8//rjz2E2bNrFw4UJWrFjh3Pf9998zceJEPvnkEzp16sRf/vIXhg0bxrp16/j222959NFHKS4u5plnniE1NZW0tLQqyyb7a916g8EbhAUHcsvF7ZzbofXIl+8U7/1KuJWxr2dfiwfuTjp27EivXr0ASE5O5uqrr0ZE6NWrFxkZGeTk5DBmzBj27NmDiDirRAJcc801tGjRwrm9Y8cO7rvvPpYtW0abNnp59rJly/j000+ZMWMGAAUFBRw4cKBOsvlj3XqDwVtEWmUR6lPZ4f4hF9Ip3l7xerCzsfciISHlt2sBAQHO7YCAAEpKSpg6dSpXXXUVixYtIiMjgyuvvNI5vnJt+9atW1NQUMDmzZudxl4pxYcffkjXrl0rjF27dm2tslWuW//EE08Aum795MmTueGGG1i+fDnTpk0759ja6tbff//9tZ7fYPAnIkO0iYwIrt1U/v2uizmeV8g9g5LcLFXDMGGcBuBaI961nHBVxMbG8vnnnzNlyhSWL18O6Drxr732Go6Ko5s3bwZqroXvwB/r1hsM3sLh2UeE1B7C+XWv1rY19GCMfYN47LHHmDJlCv369atT1k1CQgKLFy/md7/7HWvXrmXq1KkUFxfTu3dvkpOTmTp1KgBXXXUV27dvp2/fvsyfP7/a90tNTWXu3LkV4vqOuvX9+/enZcuqi0xNmDCBFStW0KdPH9asWVOhbv2dd97JoEGD6NWrF6NHj671R8dg8AdCrZXZESG+HwQx9ewN54353gxNlUOn8rnsr99w/9ALmTLCHjre0Hr2vv9zZTAYDG6iTWwY3z5yJe1bhHtblPPGGHubYurWGwz2wNHY3texnbFXSnm9g40d8JW69d4KAxoMhvphqwna0NBQsrOzjQHxEZRSZGdnExpqOkQZDHbHVp59u3btyMzMJCsry9uiGOpIaGgo7dq1q32gwWDwKrYy9kFBQfVa+WkwGAyGumGrMI7BYDAY3IMx9gaDweAHGGNvMBgMfoDXVtCKyGlgl7UZA+TU8dD6jHXQHqhbWcnzO099j7GrXOAZ2fxZLrt+j3aVqyHnaapydVVKRdXzeJ0+540HsMHl+ex6HFfnsS7HZDXgmIacp17H2FUuT8nmz3LZ9Xu0q1x2/R69IZer7azPwy5hnM/cNNbBqQYc05Dz1PcYu8oFnpHNn+Wy6/doV7kach5/l6sC3gzjbFANKOZj93PVB7vKBfaVzchVP4xc9cMX5GqojN707Gc30XPVB7vKBfaVzchVP4xc9cMX5GqQjF7z7A0Gg8HgOewSszcYDAaDGzHG3suISJi3ZTA0XYx+GRz4nbEXG9VPFpEpwCMiElLrYC9ip8/MF7DL52X0q2nS0M/Lb4y9iAQCKBtMUoiIowDdamAo0M2L4lSJiASJyFDQn5lDZnNhVo9ddMzoV9PkfPXL7yZoReRqYCTwrFLqhA3k+SsQAjyplLJFl28REesC/BewF4gFTiulnvW2TN46f32wk44Z/aq/XN6UoS40VL+atGcvIgHW30ARiRGRD9Af0lJvXYSiaSUiT4nIJcCLQF/gMm/I44rj83JR+HXAc0AU8IKXZBIRCXC9CB1y2gG76ZjRrwbJZVsda0z9ssU/1Ni43O6UiUigUqpUKZUDDATaK6WWiUiQh2R5SUSesJ7HWwp1CkgAhimlsoH3gTEi0soTMlUjZ6BSqsx6PkxEHgB+Bj4A8pRShZ76zFxkClCaMhHpLiL3ikioQ05vYhcdM/p13nLZUsfcol8NqbHgKw/gQWAJ8BgQBqQAZ1xeFw/IcAWQDXQFFgC/svZfCfwTuM7a/hgYBzTz4OfTHrgOiAYC0T/+s4AtwEhrTDSwD7i00rFu/+ys84QC49Fe4HLgVWCgJ2Wws44Z/WraOtaY+tUkPHsRuUpEOrpsXyQibwN9gFesvy+glexTEfmHNdSt/7/lNawClgHPAx8C9wAopZYDvwCjRCQY+BcwFmjhTpkcconI34AVwP3AO8ATQARwoVKqj1JqsYg0U0rlAm8Bj4tIpIj8QUSClKVpjS1Xpe1A4DXgIaXUQGAEulrgCBGJcocMNchmOx0z+tUw2Spt20LHPKJf3vzVaqRfvhbAIWAlcK+173rgLHCttd0deBq4yxpfhC4T6m7ZxEXGU8CtaMW6x9o/GH1B/j9ru4OHPrP7gIVAoLXdyZLjMUu+1kAQLl4g+oL8HngP7WG4zeMBOgOx1vNrgFygrbV9HfAycLO/65jRr6ahY57Sr6bg2ZcCm4B/AxNE5G50ytkbwO0ASqkdgAAdlZ7UeBXo527BlFLK8r5OoJXnj2jv6zER6QPcCHyLvn1EKfWziHtTz0SnuF0H/K9SqlREIpRSe4FHgDHo2+rWSqlipVSJiAwWkfZoD+1mpdSdSql8ZWlgI8jzkohMtZ53EZEF6Nof74rIQKXUl+jv9knrkFXAQeBaEWnTGDLUAVvqmNGvOstkdx3ziH75tLEXEVF60uIkEAlMAoYAD6CVPlZEbrWGN0fHDVFKPaqUet8TMiprokcp9TQQj04zm4H+soqUUvcopX50Ge/W20alVAnaK+hs7cq39s8HjgEXA2NFZLqIvAVMB2KUUkVKqSNuEGkR8JCIRAIPAcuUUlehMzRmWCGIvwL9ReRSpdRZdHhgvlLqkBvkqYDddczoV52wrY55Ur982ti78BEQpJRaD2xDf6H/BewGXhaRf6MXlvwbyhduuNvLceASJ/wjMF0pNQd9e/bnSq97QhZBK3JnK3ujTEQcXW++RV8YbwIFwA6l1FCl1FY3yeKIOa8AZiilHgA2i8j3wGb07fzvlVL70TXA/waglFqvlPq2hvdNdPlfGwvb6pjRrxrlaXQdE5FkEQltZFHdr1/ujEV56gHcjc5EmA/8hJ6IWgR8B6wF/mwDGQOsv18Bo63ngXhhth+dueGYlHLdPx+40lVeN8vhiDnHoWOmSejsg6et/ROBPKADEE4tMUr0be3X6BCCX+mY0S/36xjQGx1eWYQV3/cl/XIsq/Z1FgMzgXlKqWQAEVmJ/iWMBCaJyN+VUie9JaAq93DOoFPNUEqVekmWXSKyBJgmItFAGvq2UaE9CZQH8oyVcsacs0VkJnpS759AqJWZ0AGt6BFKqZ+BXdZtb4VQhOXdvAT8CnhRKfWOG8S1tY4Z/apWlkbRMYsngIVKqVccO2oYW1/crl9NxdjnAHOApeBcwLEP2CciCejYXEkjfjENJQWdOrXFizIAoJRaKiI56PjgOGCRUuoNL8jhiDk/YcUmBwDpwBq04b660vhzvj/rgo4ENjsMvYh0AvY3olHxBR0z+lW1LOelY1YYLAm9+OsVa981wHr0XUFjfO9u168mURvH8uw+Rv9iL3b9MGxg4J3YSRZXvC2X5XmVicjNwF+VUl1EpLnDi7EUv7TSMZcCJ5RSu63taPTFNxcYDhxFX4ivKqU2NoKMttcxu8hRGTvIVV8dq0a/NqJj6RPQsf6j6HmH5xtBPrfrV5OYoLU+iHFKqc8qfyjeVjJX7CSLK96Wy7oIA5RSHwEHRORWpdRJ0fVApNJFGCsinwNfAreJSIT1Hrno1ZmjgSnAHcBh4BYRiW8EGW2vY3aRozJ2kKuuOlaLfr0FPAu8qZS6Dm2YL7V+GM5XPrfrV5Mw9gDKKgrkiewHQ+NTKea819pXWoWiRwBfoFPUItBhAsd7zERPAK5UShWiPaUU9OKUxpDR6JgPU0cdq1a/0HH1JHQKJMAGtHdf2EjyuVW/moyxd2AHL8LQYKqMOYvIPSIyVESilVIH0QtiFqDT9y4Rl4UvlSaw+gOZ6EUrjYbRMZ/mHB2rg361BVB6vcKjwIMi0hKdQdMTXZuo0XCXfjWJmL2haeAam7S8m0T08vkytCcWAfxBKXXcGjMYuA1Yr5Saa+0LAQahFxYdBh52xF0NBoeO1VO/Niil3nV5j8nAheiFY/+tlNru4X+jQTQ5z97gu7gY+kDreRRw0MqW+C1wAu11OcZ/B2QA3UTX+g6zwjdFwHNKqeuNoTe4Yhn6+upXV0u/oqz9L6GN/HW+YujBePYGGyG6AuGz6MVAS9B1VEYrpcZYrwegC0alKqVWWPsi0Q0wBqNL6vZTHiijYPA9zlO/LkPn5PusfhnP3mALRPcj3Yie/EpHX5TFwFUiMhCc+dLTrIeD36AX7KQBvXz1QjS4l0bQry34uH41lUVVBt+nDPgfR2xURPoBHdGVCF9HF6kKQGfYDBORJKVUBnoS7VdKqZXeEdvgI/i9fhnP3mAXNgILrFtt0DVB2itd1CtQRCZZnlc7oNS6EFFKfdIULkSD2/F7/TLG3mALlFJnlVKFLguorgGyrOfjgO4ishhd9W8TmHx3Q90x+mXCOAabYXleCt0w+1Nr92ngcXRO834rF9rkuxvqjT/rl/HsDXajDN2y7jjQ2/K2pgJlSqnVjgvRYGggfqtfJvXSYDusWiPfW4+3lFL/8rJIhiaEv+qXMfYG2yEi7dBdel6yFkkZDI2Gv+qXMfYGg8HgB5iYvcFgMPgBxtgbDAaDH2CMvcFgMPgBxtgbDAaDH2CMvcFgMPgBxtgb/BYRmSYij9Tw+o0i0sOTMhkM7sIYe4Ohem4EjLE3NAlMnr3BrxCRPwNjgGPAL+hqiDnAfUAwutb5fwF90Q2mc6zHLdZbzALi0U3MJyildnpSfoOhoRhjb/AbRKQ/MAe4BF0EcBPwD/SS+WxrzHPAUaXUayIyB1islFpovfY1MFEptUdELgGeV0oN8/x/YjDUH1P10uBPXAEsUkqdBRARR9XDnpaRjwUigS8qH2i1p7sM+MCl8m2I2yU2GBoJY+wNBu3t36iU2iIiY4ErqxgTAJxSSvX1oFwGQ6NhJmgN/sRK4EYRCRORKOB6a38UcFhEgoC7XMaftl5DKZUL7BeRW0E3thCRPp4T3WA4P4yxN/gNSqlNwHx08+ilwHrrpanAWnSrOtcJ1/eBR0Vks4h0Qv8Q3CsiW4CfgFGekt1gOF/MBK3BYDD4AcazNxgMBj/AGHuDwWDwA4yxNxgMBj/AGHuDwWDwA4yxNxgMBj/AGHuDwWDwA4yxNxgMBj/AGHuDwWDwA/4PJkM4CskeQW0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# same as in https://github.com/vermouth1992/drl-portfolio-management/blob/master/src/environment/portfolio.py\n",
    "def sharpe(returns, freq=30, rfr=0):\n",
    "    \"\"\" Given a set of returns, calculates naive (rfr=0) sharpe. \"\"\"\n",
    "    eps = np.finfo(np.float32).eps\n",
    "    return (np.sqrt(freq) * np.mean(returns - rfr + eps)) / np.std(returns - rfr + eps)\n",
    "\n",
    "\n",
    "def max_drawdown(returns):\n",
    "    \"\"\" Max drawdown. See https://www.investopedia.com/terms/m/maximum-drawdown-mdd.asp \"\"\"\n",
    "    eps = np.finfo(np.float32).eps\n",
    "    peak = returns.max()\n",
    "    trough = returns[returns.argmax():].min()\n",
    "    return (trough - peak) / (peak + eps)\n",
    "\n",
    "info = info_dir + 'portfolio-management.csv'\n",
    "df_info = pd.read_csv(info)\n",
    "df_info['date'] = pd.to_datetime(df_info['date'], format='%Y-%m-%d')\n",
    "df_info.set_index('date', inplace=True)\n",
    "mdd = max_drawdown(df_info.rate_of_return + 1)\n",
    "sharpe_ratio = sharpe(df_info.rate_of_return)\n",
    "title = 'max_drawdown={: 2.2%} sharpe_ratio={: 2.4f}'.format(mdd, sharpe_ratio)\n",
    "df_info[[\"portfolio_value\", \"market_value\"]].plot(title=title, fig=plt.gcf(), rot=30)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
